{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e164a8-8551-4c44-8dab-eef32590a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with new columns added:\n",
      "   CUSTOMER_ID  STORE_NUMBER ORDER_CREATED_DATE    ORDER_ID  \\\n",
      "0    362204699          2156         2024-07-24  7247194287   \n",
      "1    269612955          1419         2025-02-15   791214421   \n",
      "2    585330633          2249         2025-02-15  7575285208   \n",
      "3    950661333          2513         2024-03-29  4253875716   \n",
      "4    434985772          1754         2024-04-08  7150407872   \n",
      "\n",
      "  ORDER_CHANNEL_NAME ORDER_SUBCHANNEL_NAME ORDER_OCCASION_NAME  \\\n",
      "0            Digital                   WWT                ToGo   \n",
      "1            Digital                   WWT                ToGo   \n",
      "2            Digital                   WWT                ToGo   \n",
      "3            Digital                   WWT                ToGo   \n",
      "4            Digital                   WWT                ToGo   \n",
      "\n",
      "                 item 1 name  item 1 price  item 1 qty  ... item 12 qty  \\\n",
      "0  10 pc Grilled Wings Combo         15.29         1.0  ...         NaN   \n",
      "1        Ranch Dip - Regular          1.59         1.0  ...         NaN   \n",
      "2      20pc Spicy Feast Deal         16.99         1.0  ...         NaN   \n",
      "3        20 pc Grilled Wings         26.59         1.0  ...         NaN   \n",
      "4   6 pc Grilled Wings Combo         11.29         1.0  ...         NaN   \n",
      "\n",
      "   item 13 name  item 13 price item 13 qty  item 14 name  item 14 price  \\\n",
      "0          None            NaN         NaN          None            NaN   \n",
      "1          None            NaN         NaN          None            NaN   \n",
      "2          None            NaN         NaN          None            NaN   \n",
      "3          None            NaN         NaN          None            NaN   \n",
      "4          None            NaN         NaN          None            NaN   \n",
      "\n",
      "  item 14 qty  item 15 name  item 15 price item 15 qty  \n",
      "0         NaN          None            NaN         NaN  \n",
      "1         NaN          None            NaN         NaN  \n",
      "2         NaN          None            NaN         NaN  \n",
      "3         NaN          None            NaN         NaN  \n",
      "4         NaN          None            NaN         NaN  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the order_data.csv file\n",
    "try:\n",
    "    order_data = pd.read_csv('order_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'order_data.csv' not found. Please make sure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Function to parse the JSON and extract a list of clean item dictionaries\n",
    "def extract_clean_items(orders_json_string):\n",
    "    if pd.isna(orders_json_string):\n",
    "        return []\n",
    "    try:\n",
    "        data = json.loads(orders_json_string)\n",
    "        clean_items = []\n",
    "        for order in data.get('orders', []):\n",
    "            for item in order.get('item_details', []):\n",
    "                # Filter out items with a price of 0\n",
    "                if item.get('item_price', 0) > 0:\n",
    "                    clean_items.append({\n",
    "                        'item name': item.get('item_name'),\n",
    "                        'item price': item.get('item_price'),\n",
    "                        'item qty': item.get('item_quantity')\n",
    "                    })\n",
    "        return clean_items\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return []\n",
    "\n",
    "# Apply the function to the 'ORDERS' column\n",
    "order_data['clean_items_list'] = order_data['ORDERS'].apply(extract_clean_items)\n",
    "\n",
    "# --- Expanding the item list into new columns ---\n",
    "# We will use a more memory-efficient approach by creating a list of new columns to add\n",
    "max_items = order_data['clean_items_list'].str.len().max()\n",
    "new_columns = {}\n",
    "\n",
    "for i in range(int(max_items)):\n",
    "    item_prefix = f'item {i+1}'\n",
    "    new_columns[f'{item_prefix} name'] = order_data['clean_items_list'].apply(lambda x: x[i]['item name'] if i < len(x) else None)\n",
    "    new_columns[f'{item_prefix} price'] = order_data['clean_items_list'].apply(lambda x: x[i]['item price'] if i < len(x) else None)\n",
    "    new_columns[f'{item_prefix} qty'] = order_data['clean_items_list'].apply(lambda x: x[i]['item qty'] if i < len(x) else None)\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "for col_name, col_data in new_columns.items():\n",
    "    order_data[col_name] = col_data\n",
    "\n",
    "# Drop the temporary 'clean_items_list' column and the original 'ORDERS' JSON column\n",
    "order_data = order_data.drop(columns=['clean_items_list', 'ORDERS'])\n",
    "\n",
    "print(\"DataFrame with new columns added:\")\n",
    "print(order_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15d06ae-d4f7-4095-8e23-b4fe15ae41d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file 'order_data_with_items.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the final DataFrame to a new CSV file\n",
    "# The 'index=False' argument prevents pandas from writing the DataFrame index to the CSV\n",
    "order_data.to_csv('order_data_with_items.csv', index=False)\n",
    "\n",
    "print(\"New CSV file 'order_data_with_items.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3f053b-91db-4e21-9895-0fe1491847df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\AppData\\Local\\Temp\\ipykernel_2164\\3425921242.py:5: DtypeWarning: Columns (34,37,40,43,46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  order_data_with_items = pd.read_csv('order_data_with_items.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame with new store details:\n",
      "   CUSTOMER_ID  STORE_NUMBER ORDER_CREATED_DATE    ORDER_ID  \\\n",
      "0    362204699          2156         2024-07-24  7247194287   \n",
      "1    269612955          1419         2025-02-15   791214421   \n",
      "2    585330633          2249         2025-02-15  7575285208   \n",
      "3    950661333          2513         2024-03-29  4253875716   \n",
      "4    434985772          1754         2024-04-08  7150407872   \n",
      "\n",
      "  ORDER_CHANNEL_NAME ORDER_SUBCHANNEL_NAME ORDER_OCCASION_NAME  \\\n",
      "0            Digital                   WWT                ToGo   \n",
      "1            Digital                   WWT                ToGo   \n",
      "2            Digital                   WWT                ToGo   \n",
      "3            Digital                   WWT                ToGo   \n",
      "4            Digital                   WWT                ToGo   \n",
      "\n",
      "                 item 1 name  item 1 price  item 1 qty  ... item 13 qty  \\\n",
      "0  10 pc Grilled Wings Combo         15.29         1.0  ...         NaN   \n",
      "1        Ranch Dip - Regular          1.59         1.0  ...         NaN   \n",
      "2      20pc Spicy Feast Deal         16.99         1.0  ...         NaN   \n",
      "3        20 pc Grilled Wings         26.59         1.0  ...         NaN   \n",
      "4   6 pc Grilled Wings Combo         11.29         1.0  ...         NaN   \n",
      "\n",
      "   item 14 name  item 14 price item 14 qty  item 15 name  item 15 price  \\\n",
      "0           NaN            NaN         NaN           NaN            NaN   \n",
      "1           NaN            NaN         NaN           NaN            NaN   \n",
      "2           NaN            NaN         NaN           NaN            NaN   \n",
      "3           NaN            NaN         NaN           NaN            NaN   \n",
      "4           NaN            NaN         NaN           NaN            NaN   \n",
      "\n",
      "  item 15 qty          CITY  STATE POSTAL_CODE  \n",
      "0         NaN     GRAPEVINE     TX       76051  \n",
      "1         NaN  HUNTERSVILLE     NC       28078  \n",
      "2         NaN           NaN    NaN       32792  \n",
      "3         NaN     LAS VEGAS     NV       89129  \n",
      "4         NaN       ARDMORE     OK       73401  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files into pandas DataFrames\n",
    "try:\n",
    "    order_data_with_items = pd.read_csv('order_data_with_items.csv')\n",
    "    store_data = pd.read_csv('store_data.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure both files are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Perform a left merge on the 'STORE_NUMBER' column\n",
    "# 'how='left'' ensures that all orders are kept, regardless of whether a store match is found\n",
    "merged_data = pd.merge(order_data_with_items, store_data, on='STORE_NUMBER', how='left')\n",
    "\n",
    "# Display the first 5 rows of the new merged DataFrame to verify the result\n",
    "print(\"Merged DataFrame with new store details:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c3213e-0a03-4842-aaec-713f46e32b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New CSV file 'merged_order_store_data.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the new DataFrame to a CSV file.\n",
    "# The 'index=False' argument prevents pandas from writing the DataFrame index as a column.\n",
    "merged_data.to_csv('merged_order_store_data.csv', index=False)\n",
    "\n",
    "print(\"\\nNew CSV file 'merged_order_store_data.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3033d524-7959-45bc-af55-a252c09bc0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\AppData\\Local\\Temp\\ipykernel_2164\\1710983525.py:5: DtypeWarning: Columns (34,37,40,43,46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_order_store_data = pd.read_csv('merged_order_store_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Merged DataFrame with customer details:\n",
      "   CUSTOMER_ID  STORE_NUMBER ORDER_CREATED_DATE    ORDER_ID  \\\n",
      "0    362204699          2156         2024-07-24  7247194287   \n",
      "1    269612955          1419         2025-02-15   791214421   \n",
      "2    585330633          2249         2025-02-15  7575285208   \n",
      "3    950661333          2513         2024-03-29  4253875716   \n",
      "4    434985772          1754         2024-04-08  7150407872   \n",
      "\n",
      "  ORDER_CHANNEL_NAME ORDER_SUBCHANNEL_NAME ORDER_OCCASION_NAME  \\\n",
      "0            Digital                   WWT                ToGo   \n",
      "1            Digital                   WWT                ToGo   \n",
      "2            Digital                   WWT                ToGo   \n",
      "3            Digital                   WWT                ToGo   \n",
      "4            Digital                   WWT                ToGo   \n",
      "\n",
      "                 item 1 name  item 1 price  item 1 qty  ... item 14 name  \\\n",
      "0  10 pc Grilled Wings Combo         15.29         1.0  ...          NaN   \n",
      "1        Ranch Dip - Regular          1.59         1.0  ...          NaN   \n",
      "2      20pc Spicy Feast Deal         16.99         1.0  ...          NaN   \n",
      "3        20 pc Grilled Wings         26.59         1.0  ...          NaN   \n",
      "4   6 pc Grilled Wings Combo         11.29         1.0  ...          NaN   \n",
      "\n",
      "   item 14 price  item 14 qty item 15 name  item 15 price  item 15 qty  \\\n",
      "0            NaN          NaN          NaN            NaN          NaN   \n",
      "1            NaN          NaN          NaN            NaN          NaN   \n",
      "2            NaN          NaN          NaN            NaN          NaN   \n",
      "3            NaN          NaN          NaN            NaN          NaN   \n",
      "4            NaN          NaN          NaN            NaN          NaN   \n",
      "\n",
      "           CITY  STATE  POSTAL_CODE CUSTOMER_TYPE  \n",
      "0     GRAPEVINE     TX        76051    Registered  \n",
      "1  HUNTERSVILLE     NC        28078    Registered  \n",
      "2           NaN    NaN        32792         Guest  \n",
      "3     LAS VEGAS     NV        89129    Registered  \n",
      "4       ARDMORE     OK        73401         Guest  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged order-store data and the customer data\n",
    "try:\n",
    "    merged_order_store_data = pd.read_csv('merged_order_store_data.csv')\n",
    "    customer_data = pd.read_csv('customer_data.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure both files are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Perform a left merge on the 'CUSTOMER_ID' column\n",
    "# This keeps all rows from the order data and adds the 'CUSTOMER_TYPE'\n",
    "final_merged_data = pd.merge(merged_order_store_data, customer_data, on='CUSTOMER_ID', how='left')\n",
    "\n",
    "# Display the first 5 rows to verify the new 'CUSTOMER_TYPE' column has been added\n",
    "print(\"Final Merged DataFrame with customer details:\")\n",
    "print(final_merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3686bba4-10d1-4bb3-b4bd-d53d1b320065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New CSV file 'final_merged_data.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the final DataFrame to a CSV file.\n",
    "final_merged_data.to_csv('final_merged_data.csv', index=False)\n",
    "\n",
    "print(\"\\nNew CSV file 'final_merged_data.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf86e9f-81af-4a20-98db-a66ed5f678b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\AppData\\Local\\Temp\\ipykernel_2164\\4065529791.py:5: DtypeWarning: Columns (34,37,40,43,46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_merged_data = pd.read_csv('final_merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 distinct items.\n",
      "Sample of distinct items: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final merged data file\n",
    "try:\n",
    "    final_merged_data = pd.read_csv('final_merged_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_merged_data.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Find all columns that contain 'item name'\n",
    "item_name_columns = [col for col in final_merged_data.columns if 'item name' in col]\n",
    "\n",
    "# Use a set to automatically handle duplicates\n",
    "all_unique_items = set()\n",
    "\n",
    "# Iterate through the identified columns and add items to the set\n",
    "for col in item_name_columns:\n",
    "    # Use dropna() to ignore any empty cells\n",
    "    unique_items_in_col = final_merged_data[col].dropna().unique()\n",
    "    all_unique_items.update(unique_items_in_col)\n",
    "\n",
    "# Convert the set of unique items to a list\n",
    "list_of_items = sorted(list(all_unique_items))\n",
    "\n",
    "print(f\"Found {len(list_of_items)} distinct items.\")\n",
    "print(\"Sample of distinct items:\", list_of_items[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f4ea5f-0175-43ff-a886-7c050eaf9edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\AppData\\Local\\Temp\\ipykernel_2164\\914177020.py:5: DtypeWarning: Columns (34,37,40,43,46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_merged_data = pd.read_csv('final_merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns in your DataFrame:\n",
      "['CUSTOMER_ID', 'STORE_NUMBER', 'ORDER_CREATED_DATE', 'ORDER_ID', 'ORDER_CHANNEL_NAME', 'ORDER_SUBCHANNEL_NAME', 'ORDER_OCCASION_NAME', 'item 1 name', 'item 1 price', 'item 1 qty', 'item 2 name', 'item 2 price', 'item 2 qty', 'item 3 name', 'item 3 price', 'item 3 qty', 'item 4 name', 'item 4 price', 'item 4 qty', 'item 5 name', 'item 5 price', 'item 5 qty', 'item 6 name', 'item 6 price', 'item 6 qty', 'item 7 name', 'item 7 price', 'item 7 qty', 'item 8 name', 'item 8 price', 'item 8 qty', 'item 9 name', 'item 9 price', 'item 9 qty', 'item 10 name', 'item 10 price', 'item 10 qty', 'item 11 name', 'item 11 price', 'item 11 qty', 'item 12 name', 'item 12 price', 'item 12 qty', 'item 13 name', 'item 13 price', 'item 13 qty', 'item 14 name', 'item 14 price', 'item 14 qty', 'item 15 name', 'item 15 price', 'item 15 qty', 'CITY', 'STATE', 'POSTAL_CODE', 'CUSTOMER_TYPE']\n",
      "\n",
      "Columns identified as item names:\n",
      "['item 1 name', 'item 2 name', 'item 3 name', 'item 4 name', 'item 5 name', 'item 6 name', 'item 7 name', 'item 8 name', 'item 9 name', 'item 10 name', 'item 11 name', 'item 12 name', 'item 13 name', 'item 14 name', 'item 15 name']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final merged data file\n",
    "try:\n",
    "    final_merged_data = pd.read_csv('final_merged_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_merged_data.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Print all column names to inspect them\n",
    "print(\"All columns in your DataFrame:\")\n",
    "print(final_merged_data.columns.tolist())\n",
    "\n",
    "# Find all columns that contain 'item' and 'name'\n",
    "item_name_columns = [col for col in final_merged_data.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "\n",
    "print(\"\\nColumns identified as item names:\")\n",
    "print(item_name_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa1de1a-ed75-4cc8-b154-2289fd5a7fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 130 distinct items.\n",
      "Sample of distinct items: ['$19.99 Crispy Feast', '10 pc Grilled Wings', '10 pc Grilled Wings Combo', '10 pc Mixed Wings', '10 pc Mixed Wings Combo', '10 pc Spicy Wings', '10 pc Spicy Wings Combo', '100 pc Family Grilled Wings', '100 pc Family Mixed Wings', '100 pc Family Spicy Wings']\n",
      "\n",
      "New CSV file 'available_items.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use a set to automatically handle duplicates from across all item columns\n",
    "all_unique_items = set()\n",
    "\n",
    "# Iterate through the identified item name columns and add items to the set\n",
    "for col in item_name_columns:\n",
    "    # Use dropna() to ignore any empty cells (NaNs)\n",
    "    unique_items_in_col = final_merged_data[col].dropna().unique()\n",
    "    all_unique_items.update(unique_items_in_col)\n",
    "\n",
    "# Convert the set of unique items to a list and sort it\n",
    "list_of_items = sorted(list(all_unique_items))\n",
    "\n",
    "print(f\"\\nFound {len(list_of_items)} distinct items.\")\n",
    "print(\"Sample of distinct items:\", list_of_items[:10])\n",
    "\n",
    "# Create a new DataFrame from the list of unique items\n",
    "available_items_df = pd.DataFrame(list_of_items, columns=['available_items'])\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "available_items_df.to_csv('available_items.csv', index=False)\n",
    "\n",
    "print(\"\\nNew CSV file 'available_items.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda0ef83-d49f-48e5-99ce-8ab0d6d4db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\AppData\\Local\\Temp\\ipykernel_2164\\312790446.py:5: DtypeWarning: Columns (34,37,40,43,46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_merged_data = pd.read_csv('final_merged_data.csv')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"2024-07-24\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m     exit()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# --- Data Type Conversion ---\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert 'ORDER_CREATED_DATE' to a datetime object\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m final_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_CREATED_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(final_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_CREATED_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# --- Data Cleaning ---\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# For categorical columns, we can fill any missing values with a placeholder like 'Unknown'\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# This prevents errors in later analysis and preserves rows with missing data\u001b[39;00m\n\u001b[0;32m     17\u001b[0m final_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUSTOMER_TYPE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUSTOMER_TYPE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m convert_listlike(unique_dates, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    457\u001b[0m     arg,\n\u001b[0;32m    458\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"2024-07-24\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final merged data file\n",
    "try:\n",
    "    final_merged_data = pd.read_csv('final_merged_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_merged_data.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Data Type Conversion ---\n",
    "# Convert 'ORDER_CREATED_DATE' to a datetime object\n",
    "final_merged_data['ORDER_CREATED_DATE'] = pd.to_datetime(final_merged_data['ORDER_CREATED_DATE'], format='%d-%m-%Y')\n",
    "\n",
    "# --- Data Cleaning ---\n",
    "# For categorical columns, we can fill any missing values with a placeholder like 'Unknown'\n",
    "# This prevents errors in later analysis and preserves rows with missing data\n",
    "final_merged_data['CUSTOMER_TYPE'] = final_merged_data['CUSTOMER_TYPE'].fillna('Unknown')\n",
    "final_merged_data['CITY'] = final_merged_data['CITY'].fillna('Unknown')\n",
    "final_merged_data['STATE'] = final_merged_data['STATE'].fillna('Unknown')\n",
    "\n",
    "# Check the data types after conversion\n",
    "print(\"Data types after conversion:\")\n",
    "print(final_merged_data.info())\n",
    "\n",
    "# Display the first few rows to show the result\n",
    "print(\"\\nDataFrame head after cleaning:\")\n",
    "print(final_merged_data.head())\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "final_merged_data.to_csv('final_merged_data_cleaned.csv', index=False)\n",
    "print(\"\\nNew CSV file 'final_merged_data_cleaned.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b3585b3-230e-41bc-9db5-5848fa438b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after conversion:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1414410 entries, 0 to 1414409\n",
      "Data columns (total 56 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   CUSTOMER_ID            1414410 non-null  int64         \n",
      " 1   STORE_NUMBER           1414410 non-null  int64         \n",
      " 2   ORDER_CREATED_DATE     1414410 non-null  datetime64[ns]\n",
      " 3   ORDER_ID               1414410 non-null  int64         \n",
      " 4   ORDER_CHANNEL_NAME     1414410 non-null  object        \n",
      " 5   ORDER_SUBCHANNEL_NAME  1414410 non-null  object        \n",
      " 6   ORDER_OCCASION_NAME    1414410 non-null  object        \n",
      " 7   item 1 name            1411675 non-null  object        \n",
      " 8   item 1 price           1411675 non-null  float64       \n",
      " 9   item 1 qty             1411675 non-null  float64       \n",
      " 10  item 2 name            812560 non-null   object        \n",
      " 11  item 2 price           812560 non-null   float64       \n",
      " 12  item 2 qty             812560 non-null   float64       \n",
      " 13  item 3 name            372360 non-null   object        \n",
      " 14  item 3 price           372360 non-null   float64       \n",
      " 15  item 3 qty             372360 non-null   float64       \n",
      " 16  item 4 name            142760 non-null   object        \n",
      " 17  item 4 price           142760 non-null   float64       \n",
      " 18  item 4 qty             142760 non-null   float64       \n",
      " 19  item 5 name            49509 non-null    object        \n",
      " 20  item 5 price           49509 non-null    float64       \n",
      " 21  item 5 qty             49509 non-null    float64       \n",
      " 22  item 6 name            16070 non-null    object        \n",
      " 23  item 6 price           16070 non-null    float64       \n",
      " 24  item 6 qty             16070 non-null    float64       \n",
      " 25  item 7 name            5102 non-null     object        \n",
      " 26  item 7 price           5102 non-null     float64       \n",
      " 27  item 7 qty             5102 non-null     float64       \n",
      " 28  item 8 name            1599 non-null     object        \n",
      " 29  item 8 price           1599 non-null     float64       \n",
      " 30  item 8 qty             1599 non-null     float64       \n",
      " 31  item 9 name            506 non-null      object        \n",
      " 32  item 9 price           506 non-null      float64       \n",
      " 33  item 9 qty             506 non-null      float64       \n",
      " 34  item 10 name           185 non-null      object        \n",
      " 35  item 10 price          185 non-null      float64       \n",
      " 36  item 10 qty            185 non-null      float64       \n",
      " 37  item 11 name           72 non-null       object        \n",
      " 38  item 11 price          72 non-null       float64       \n",
      " 39  item 11 qty            72 non-null       float64       \n",
      " 40  item 12 name           30 non-null       object        \n",
      " 41  item 12 price          30 non-null       float64       \n",
      " 42  item 12 qty            30 non-null       float64       \n",
      " 43  item 13 name           13 non-null       object        \n",
      " 44  item 13 price          13 non-null       float64       \n",
      " 45  item 13 qty            13 non-null       float64       \n",
      " 46  item 14 name           6 non-null        object        \n",
      " 47  item 14 price          6 non-null        float64       \n",
      " 48  item 14 qty            6 non-null        float64       \n",
      " 49  item 15 name           2 non-null        object        \n",
      " 50  item 15 price          2 non-null        float64       \n",
      " 51  item 15 qty            2 non-null        float64       \n",
      " 52  CITY                   1414410 non-null  object        \n",
      " 53  STATE                  1414410 non-null  object        \n",
      " 54  POSTAL_CODE            1320762 non-null  object        \n",
      " 55  CUSTOMER_TYPE          1414410 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(30), int64(3), object(22)\n",
      "memory usage: 604.3+ MB\n",
      "None\n",
      "\n",
      "DataFrame head after cleaning:\n",
      "   CUSTOMER_ID  STORE_NUMBER ORDER_CREATED_DATE    ORDER_ID  \\\n",
      "0    362204699          2156         2024-07-24  7247194287   \n",
      "1    269612955          1419         2025-02-15   791214421   \n",
      "2    585330633          2249         2025-02-15  7575285208   \n",
      "3    950661333          2513         2024-03-29  4253875716   \n",
      "4    434985772          1754         2024-04-08  7150407872   \n",
      "\n",
      "  ORDER_CHANNEL_NAME ORDER_SUBCHANNEL_NAME ORDER_OCCASION_NAME  \\\n",
      "0            Digital                   WWT                ToGo   \n",
      "1            Digital                   WWT                ToGo   \n",
      "2            Digital                   WWT                ToGo   \n",
      "3            Digital                   WWT                ToGo   \n",
      "4            Digital                   WWT                ToGo   \n",
      "\n",
      "                 item 1 name  item 1 price  item 1 qty  ... item 14 name  \\\n",
      "0  10 pc Grilled Wings Combo         15.29         1.0  ...          NaN   \n",
      "1        Ranch Dip - Regular          1.59         1.0  ...          NaN   \n",
      "2      20pc Spicy Feast Deal         16.99         1.0  ...          NaN   \n",
      "3        20 pc Grilled Wings         26.59         1.0  ...          NaN   \n",
      "4   6 pc Grilled Wings Combo         11.29         1.0  ...          NaN   \n",
      "\n",
      "   item 14 price  item 14 qty item 15 name  item 15 price  item 15 qty  \\\n",
      "0            NaN          NaN          NaN            NaN          NaN   \n",
      "1            NaN          NaN          NaN            NaN          NaN   \n",
      "2            NaN          NaN          NaN            NaN          NaN   \n",
      "3            NaN          NaN          NaN            NaN          NaN   \n",
      "4            NaN          NaN          NaN            NaN          NaN   \n",
      "\n",
      "           CITY    STATE  POSTAL_CODE CUSTOMER_TYPE  \n",
      "0     GRAPEVINE       TX        76051    Registered  \n",
      "1  HUNTERSVILLE       NC        28078    Registered  \n",
      "2       Unknown  Unknown        32792         Guest  \n",
      "3     LAS VEGAS       NV        89129    Registered  \n",
      "4       ARDMORE       OK        73401         Guest  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "\n",
      "New CSV file 'final_merged_data_cleaned.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final merged data file\n",
    "# Added low_memory=False to handle the DtypeWarning\n",
    "try:\n",
    "    final_merged_data = pd.read_csv('final_merged_data.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_merged_data.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Data Type Conversion ---\n",
    "# Convert 'ORDER_CREATED_DATE' to a datetime object using the correct format\n",
    "final_merged_data['ORDER_CREATED_DATE'] = pd.to_datetime(final_merged_data['ORDER_CREATED_DATE'], format='%Y-%m-%d')\n",
    "\n",
    "# --- Data Cleaning ---\n",
    "# Fill any missing values in categorical columns with a placeholder\n",
    "final_merged_data['CUSTOMER_TYPE'] = final_merged_data['CUSTOMER_TYPE'].fillna('Unknown')\n",
    "final_merged_data['CITY'] = final_merged_data['CITY'].fillna('Unknown')\n",
    "final_merged_data['STATE'] = final_merged_data['STATE'].fillna('Unknown')\n",
    "\n",
    "# Check the data types after conversion\n",
    "print(\"Data types after conversion:\")\n",
    "print(final_merged_data.info())\n",
    "\n",
    "# Display the first few rows to show the result\n",
    "print(\"\\nDataFrame head after cleaning:\")\n",
    "print(final_merged_data.head())\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "final_merged_data.to_csv('final_merged_data_cleaned.csv', index=False)\n",
    "print(\"\\nNew CSV file 'final_merged_data_cleaned.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd4edf65-877b-4f84-b1f6-c91d8c792b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates across all columns\n",
    "num_duplicates = final_merged_data.duplicated().sum()\n",
    "if num_duplicates > 0:\n",
    "    print(f\"Found and removed {num_duplicates} duplicate rows.\")\n",
    "    final_merged_data.drop_duplicates(inplace=True)\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5747a2-016f-4925-8b95-fdc3c57c5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns that are of object type and should be standardized\n",
    "string_cols = final_merged_data.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert string columns to a uniform format\n",
    "for col in string_cols:\n",
    "    final_merged_data[col] = final_merged_data[col].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "463ac130-5ea1-4399-8e47-22a815ad41cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'final_dataset_with_features.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data from the previous step\n",
    "try:\n",
    "    final_merged_data_cleaned = pd.read_csv('final_merged_data_cleaned.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_merged_data_cleaned.csv' not found. Please run the data cleaning step first.\")\n",
    "    exit()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# 1. Total Order Value\n",
    "item_price_columns = [col for col in final_merged_data_cleaned.columns if 'item price' in col.lower()]\n",
    "final_merged_data_cleaned['total_order_value'] = final_merged_data_cleaned[item_price_columns].sum(axis=1)\n",
    "\n",
    "# 2. Order Time Features\n",
    "final_merged_data_cleaned['ORDER_CREATED_DATE'] = pd.to_datetime(final_merged_data_cleaned['ORDER_CREATED_DATE'])\n",
    "final_merged_data_cleaned['order_day_of_week'] = final_merged_data_cleaned['ORDER_CREATED_DATE'].dt.day_name()\n",
    "final_merged_data_cleaned['order_month'] = final_merged_data_cleaned['ORDER_CREATED_DATE'].dt.month\n",
    "final_merged_data_cleaned['order_hour'] = final_merged_data_cleaned['ORDER_CREATED_DATE'].dt.hour\n",
    "\n",
    "# 3. Customer Loyalty Features\n",
    "customer_order_counts = final_merged_data_cleaned.groupby('CUSTOMER_ID')['ORDER_ID'].transform('count')\n",
    "final_merged_data_cleaned['total_orders_per_customer'] = customer_order_counts\n",
    "\n",
    "# Save the final DataFrame with all new features to a new CSV file\n",
    "final_merged_data_cleaned.to_csv('final_dataset_with_features.csv', index=False)\n",
    "print(\"The file 'final_dataset_with_features.csv' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47aac5be-b898-4c17-abf4-afd9af2a38cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'final_dataset_with_features.csv' has been created and saved to your working directory.\n"
     ]
    }
   ],
   "source": [
    "# To save the DataFrame to a new CSV file\n",
    "final_merged_data_cleaned.to_csv('final_dataset_with_features.csv', index=False)\n",
    "\n",
    "print(\"The file 'final_dataset_with_features.csv' has been created and saved to your working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1ab2309-6e42-4a45-8aa2-8ec0ac26a599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with corrected order value and total spend:\n",
      "   CUSTOMER_ID    ORDER_ID  total_order_value  customer_total_spend\n",
      "0    362204699  7247194287              39.57                 39.57\n",
      "1    269612955   791214421              70.57                 70.57\n",
      "2    585330633  7575285208              16.99                 16.99\n",
      "3    950661333  4253875716              28.08                 28.08\n",
      "4    434985772  7150407872              24.58                 24.58\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the previous step\n",
    "try:\n",
    "    final_merged_data_cleaned = pd.read_csv('final_dataset_with_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_features.csv' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Identify all columns that contain 'item' and 'price'\n",
    "item_price_columns = [col for col in final_merged_data_cleaned.columns if 'item' in col.lower() and 'price' in col.lower()]\n",
    "\n",
    "# Convert each identified price column to a numeric type\n",
    "for col in item_price_columns:\n",
    "    final_merged_data_cleaned[col] = pd.to_numeric(final_merged_data_cleaned[col], errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate 'total_order_value' by summing across the item price columns\n",
    "final_merged_data_cleaned['total_order_value'] = final_merged_data_cleaned[item_price_columns].sum(axis=1)\n",
    "\n",
    "# Recalculate 'customer_total_spend' using the corrected 'total_order_value'\n",
    "customer_total_spend = final_merged_data_cleaned.groupby('CUSTOMER_ID')['total_order_value'].transform('sum')\n",
    "final_merged_data_cleaned['customer_total_spend'] = customer_total_spend\n",
    "\n",
    "print(\"DataFrame with corrected order value and total spend:\")\n",
    "print(final_merged_data_cleaned[['CUSTOMER_ID', 'ORDER_ID', 'total_order_value', 'customer_total_spend']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d91ef8-b1b9-4478-aaf3-fd4ad29d5070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'final_dataset_with_advanced_features.csv' has been created and saved to your working directory.\n"
     ]
    }
   ],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "final_merged_data_cleaned.to_csv('final_dataset_with_advanced_features.csv', index=False)\n",
    "\n",
    "print(\"The file 'final_dataset_with_advanced_features.csv' has been created and saved to your working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecaa0bcb-4982-4559-a23b-700b37a533be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with 'order_size' feature:\n",
      "     ORDER_ID  order_size\n",
      "0  7247194287           3\n",
      "1   791214421           3\n",
      "2  7575285208           1\n",
      "3  4253875716           2\n",
      "4  7150407872           2\n"
     ]
    }
   ],
   "source": [
    "# Identify all columns that contain 'item' and 'name'\n",
    "item_name_columns = [col for col in final_merged_data_cleaned.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "\n",
    "# Calculate 'order_size' by counting non-null item names in each row\n",
    "final_merged_data_cleaned['order_size'] = final_merged_data_cleaned[item_name_columns].count(axis=1)\n",
    "\n",
    "print(\"DataFrame with 'order_size' feature:\")\n",
    "print(final_merged_data_cleaned[['ORDER_ID', 'order_size']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a9b61c1-331e-452a-af4e-136a12d94b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming 'ORDER_CREATED_DATE' is already a datetime object\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# If not, run this line first:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# final_merged_data_cleaned['ORDER_CREATED_DATE'] = pd.to_datetime(final_merged_data_cleaned['ORDER_CREATED_DATE'])\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m final_merged_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_day_of_week\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_merged_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_CREATED_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday_name()\n\u001b[0;32m      6\u001b[0m final_merged_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_hour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_merged_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_CREATED_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataFrame with time-based features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6297\u001b[0m ):\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Assuming 'ORDER_CREATED_DATE' is already a datetime object\n",
    "# If not, run this line first:\n",
    "# final_merged_data_cleaned['ORDER_CREATED_DATE'] = pd.to_datetime(final_merged_data_cleaned['ORDER_CREATED_DATE'])\n",
    "\n",
    "final_merged_data_cleaned['order_day_of_week'] = final_merged_data_cleaned['ORDER_CREATED_DATE'].dt.day_name()\n",
    "final_merged_data_cleaned['order_hour'] = final_merged_data_cleaned['ORDER_CREATED_DATE'].dt.hour\n",
    "\n",
    "print(\"\\nDataFrame with time-based features:\")\n",
    "print(final_merged_data_cleaned[['ORDER_ID', 'order_day_of_week', 'order_hour']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a74b75-4da0-4d7c-845b-0dd97867aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with time-based features:\n",
      "  ORDER_CREATED_DATE order_day_of_week  order_hour\n",
      "0         2024-07-24         Wednesday           0\n",
      "1         2025-02-15          Saturday           0\n",
      "2         2025-02-15          Saturday           0\n",
      "3         2024-03-29            Friday           0\n",
      "4         2024-04-08            Monday           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file you've saved from the previous step\n",
    "try:\n",
    "    final_merged_data_cleaned = pd.read_csv('final_dataset_with_advanced_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_advanced_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# --- Corrected Order Time Features Code ---\n",
    "\n",
    "# Ensure the 'ORDER_CREATED_DATE' column is converted to a datetime object first\n",
    "final_merged_data_cleaned['ORDER_CREATED_DATE'] = pd.to_datetime(final_merged_data_cleaned['ORDER_CREATED_DATE'], format='%Y-%m-%d')\n",
    "\n",
    "# Now, create the time-based features\n",
    "final_merged_data_cleaned['order_day_of_week'] = final_merged_data_cleaned['ORDER_CREATED_DATE'].dt.day_name()\n",
    "final_merged_data_cleaned['order_hour'] = final_merged_data_cleaned['ORDER_CREATED_DATE'].dt.hour\n",
    "\n",
    "print(\"DataFrame with time-based features:\")\n",
    "print(final_merged_data_cleaned[['ORDER_CREATED_DATE', 'order_day_of_week', 'order_hour']].head())\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "final_merged_data_cleaned.to_csv('final_dataset_with_advanced_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b65a963-ec42-4e8b-b465-3f3107a97aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with 'recency_days' feature:\n",
      "   CUSTOMER_ID  recency_days\n",
      "0    362204699           279\n",
      "1    269612955            73\n",
      "2    585330633            73\n",
      "3    950661333           396\n",
      "4    434985772           386\n"
     ]
    }
   ],
   "source": [
    "# Find the most recent date in the dataset\n",
    "latest_date = final_merged_data_cleaned['ORDER_CREATED_DATE'].max()\n",
    "\n",
    "# Calculate recency for each customer\n",
    "recency_df = final_merged_data_cleaned.groupby('CUSTOMER_ID')['ORDER_CREATED_DATE'].max().reset_index()\n",
    "recency_df['recency_days'] = (latest_date - recency_df['ORDER_CREATED_DATE']).dt.days\n",
    "\n",
    "# Merge the recency feature back into the main DataFrame\n",
    "final_merged_data_cleaned = pd.merge(final_merged_data_cleaned, recency_df[['CUSTOMER_ID', 'recency_days']], on='CUSTOMER_ID', how='left')\n",
    "\n",
    "print(\"\\nDataFrame with 'recency_days' feature:\")\n",
    "print(final_merged_data_cleaned[['CUSTOMER_ID', 'recency_days']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "593da931-e077-4922-b7ca-e2d6c5d74038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with one-hot encoded features:\n",
      "   customer_type_Deleted Account  customer_type_Guest  customer_type_Online  \\\n",
      "0                          False                False                 False   \n",
      "1                          False                False                 False   \n",
      "2                          False                 True                 False   \n",
      "3                          False                False                 False   \n",
      "4                          False                 True                 False   \n",
      "\n",
      "   customer_type_Registered  customer_type_Unknown  customer_type_eClub  \n",
      "0                      True                  False                False  \n",
      "1                      True                  False                False  \n",
      "2                     False                  False                False  \n",
      "3                      True                  False                False  \n",
      "4                     False                  False                False  \n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the 'CUSTOMER_TYPE' column\n",
    "customer_type_dummies = pd.get_dummies(final_merged_data_cleaned['CUSTOMER_TYPE'], prefix='customer_type')\n",
    "\n",
    "# Concatenate the new columns with the main DataFrame\n",
    "final_merged_data_cleaned = pd.concat([final_merged_data_cleaned, customer_type_dummies], axis=1)\n",
    "\n",
    "print(\"\\nDataFrame with one-hot encoded features:\")\n",
    "print(final_merged_data_cleaned[[col for col in final_merged_data_cleaned.columns if 'customer_type' in col]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97b1a57b-6a12-4370-bf20-d9faf2d06127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with all new features added:\n",
      "   CUSTOMER_ID    ORDER_ID  customer_frequency  average_order_value  \\\n",
      "0    362204699  7247194287                   1                39.57   \n",
      "1    269612955   791214421                   1                70.57   \n",
      "2    585330633  7575285208                   1                16.99   \n",
      "3    950661333  4253875716                   1                28.08   \n",
      "4    434985772  7150407872                   1                24.58   \n",
      "\n",
      "   recency_days  order_size order_day_of_week  order_hour  \n",
      "0           279           3         Wednesday           0  \n",
      "1            73           3          Saturday           0  \n",
      "2            73           1          Saturday           0  \n",
      "3           396           2            Friday           0  \n",
      "4           386           2            Monday           0  \n",
      "\n",
      "New CSV file 'final_dataset_with_all_features.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file from the last successful step\n",
    "try:\n",
    "    final_merged_data = pd.read_csv('final_dataset_with_advanced_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_advanced_features.csv' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure the 'ORDER_CREATED_DATE' column is in datetime format\n",
    "final_merged_data['ORDER_CREATED_DATE'] = pd.to_datetime(final_merged_data['ORDER_CREATED_DATE'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "# --- 1. Monetary and Behavioral Features ---\n",
    "\n",
    "# Customer Frequency: The total number of orders per customer\n",
    "customer_frequency = final_merged_data.groupby('CUSTOMER_ID')['ORDER_ID'].transform('count')\n",
    "final_merged_data['customer_frequency'] = customer_frequency\n",
    "\n",
    "# Average Order Value: The average amount a customer spends per order\n",
    "average_order_value = final_merged_data.groupby('CUSTOMER_ID')['total_order_value'].transform('mean')\n",
    "final_merged_data['average_order_value'] = average_order_value\n",
    "\n",
    "\n",
    "# --- 2. Temporal Features ---\n",
    "\n",
    "# Customer Recency: Days since the last order for each customer\n",
    "latest_date = final_merged_data['ORDER_CREATED_DATE'].max()\n",
    "recency_df = final_merged_data.groupby('CUSTOMER_ID')['ORDER_CREATED_DATE'].max().reset_index()\n",
    "recency_df['recency_days'] = (latest_date - recency_df['ORDER_CREATED_DATE']).dt.days\n",
    "\n",
    "# Merge the recency feature back into the main DataFrame\n",
    "final_merged_data = pd.merge(final_merged_data, recency_df[['CUSTOMER_ID', 'recency_days']], on='CUSTOMER_ID', how='left')\n",
    "\n",
    "\n",
    "# --- 3. Item-Level Behavioral Features ---\n",
    "\n",
    "# Order Size: Count of distinct items in each order\n",
    "item_name_columns = [col for col in final_merged_data.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_merged_data['order_size'] = final_merged_data[item_name_columns].count(axis=1)\n",
    "\n",
    "\n",
    "# --- 4. Contextual Features ---\n",
    "\n",
    "# Time-of-Day Features: Day of the week and hour of the day\n",
    "final_merged_data['order_day_of_week'] = final_merged_data['ORDER_CREATED_DATE'].dt.day_name()\n",
    "final_merged_data['order_hour'] = final_merged_data['ORDER_CREATED_DATE'].dt.hour\n",
    "\n",
    "\n",
    "print(\"DataFrame with all new features added:\")\n",
    "print(final_merged_data[['CUSTOMER_ID', 'ORDER_ID', 'customer_frequency', 'average_order_value', 'recency_days', 'order_size', 'order_day_of_week', 'order_hour']].head())\n",
    "\n",
    "# Save the final DataFrame with all new features to a new CSV file\n",
    "final_merged_data.to_csv('final_dataset_with_all_features.csv', index=False)\n",
    "print(\"\\nNew CSV file 'final_dataset_with_all_features.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e34b4aa-7e0e-48ee-bfbe-42461bb6758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the file with all engineered features and the 'ITEMS_LIST' column\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     final_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_dataset_with_all_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_dataset_with_all_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the file with all engineered features and the 'ITEMS_LIST' column\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column from the item name columns, as it's not saved to CSV\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "transactions = final_dataset_filtered['ITEMS_LIST'].tolist()\n",
    "\n",
    "# --- 1. Split the transactions for validation ---\n",
    "train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 2. Train the model on the training set ---\n",
    "te = TransactionEncoder()\n",
    "te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets_train = apriori(df_one_hot_train, min_support=0.005, use_colnames=True)\n",
    "rules_train = association_rules(frequent_itemsets_train, metric=\"confidence\", min_threshold=0.5)\n",
    "rules_train.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "# --- 3. Validate the model on the test set ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "\n",
    "    for order in test_transactions:\n",
    "        # Simulate a missing item by hiding the last item\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "\n",
    "        if not simulated_cart:\n",
    "            continue\n",
    "\n",
    "        total_queries += 1\n",
    "\n",
    "        # Get recommendations for the simulated cart\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "\n",
    "        # Check if the missing item is in the top recommendations\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "recall_score = calculate_recall_at_k(rules_train, test_transactions)\n",
    "print(f\"\\nModel Recall@3 on the simulated test set: {recall_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfe0056f-72ee-4798-8f8f-b9d4f9e6d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and processed 1411675 transactions.\n",
      "The first 5 transactions are:\n",
      "[['10 pc Grilled Wings Combo', '8 pc Grilled Wings Combo', '8 pc Spicy Wings Combo'], ['Ranch Dip - Regular', '50 pc Grilled Wings', 'Regular Buffalo Fries'], ['20pc Spicy Feast Deal'], ['20 pc Grilled Wings', 'Ranch Dip - Regular'], ['6 pc Grilled Wings Combo', '8 pc Grilled Wings Combo']]\n",
      "\n",
      "One-hot encoded data head:\n",
      "   $19.99 Crispy Feast  10 pc Grilled Wings  10 pc Grilled Wings Combo  \\\n",
      "0                False                False                       True   \n",
      "1                False                False                      False   \n",
      "2                False                False                      False   \n",
      "3                False                False                      False   \n",
      "4                False                False                      False   \n",
      "\n",
      "   10 pc Mixed Wings  10 pc Mixed Wings Combo  10 pc Spicy Wings  \\\n",
      "0              False                    False              False   \n",
      "1              False                    False              False   \n",
      "2              False                    False              False   \n",
      "3              False                    False              False   \n",
      "4              False                    False              False   \n",
      "\n",
      "   10 pc Spicy Wings Combo  100 pc Family Grilled Wings  \\\n",
      "0                    False                        False   \n",
      "1                    False                        False   \n",
      "2                    False                        False   \n",
      "3                    False                        False   \n",
      "4                    False                        False   \n",
      "\n",
      "   100 pc Family Mixed Wings  100 pc Family Spicy Wings  ...  Seasoning Pack  \\\n",
      "0                      False                      False  ...           False   \n",
      "1                      False                      False  ...           False   \n",
      "2                      False                      False  ...           False   \n",
      "3                      False                      False  ...           False   \n",
      "4                      False                      False  ...           False   \n",
      "\n",
      "   Sports Drink  Sub Box  Triple Chocolate Cake  Triple Feast Deal  \\\n",
      "0         False    False                  False              False   \n",
      "1         False    False                  False              False   \n",
      "2         False    False                  False              False   \n",
      "3         False    False                  False              False   \n",
      "4         False    False                  False              False   \n",
      "\n",
      "   Veggie Sticks  Veggie Sticks Spicy  Voodoo Fries - Large  \\\n",
      "0          False                False                 False   \n",
      "1          False                False                 False   \n",
      "2          False                False                 False   \n",
      "3          False                False                 False   \n",
      "4          False                False                 False   \n",
      "\n",
      "   Voodoo Fries - Regular  Wings Lunch Combo  \n",
      "0                   False              False  \n",
      "1                   False              False  \n",
      "2                   False              False  \n",
      "3                   False              False  \n",
      "4                   False              False  \n",
      "\n",
      "[5 rows x 130 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'final_dataset_with_all_features.csv'\n",
    "chunk_size = 10000  # You can adjust this size based on your system's memory\n",
    "\n",
    "# Create a list to store the 'ITEMS_LIST' from each chunk\n",
    "all_transactions = []\n",
    "item_name_columns = None\n",
    "\n",
    "try:\n",
    "    # Read the CSV in chunks\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):\n",
    "        # Identify item name columns on the first chunk\n",
    "        if item_name_columns is None:\n",
    "            item_name_columns = [col for col in chunk.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "        \n",
    "        # Create 'ITEMS_LIST' for each chunk\n",
    "        chunk['ITEMS_LIST'] = chunk[item_name_columns].apply(\n",
    "            lambda row: [item for item in row.dropna()], axis=1\n",
    "        )\n",
    "        \n",
    "        # Filter for non-empty item lists and append to the main list\n",
    "        all_transactions.extend(chunk[chunk['ITEMS_LIST'].str.len() > 0]['ITEMS_LIST'].tolist())\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Successfully loaded and processed {len(all_transactions)} transactions.\")\n",
    "print(\"The first 5 transactions are:\")\n",
    "print(all_transactions[:5])\n",
    "\n",
    "# --- Preparing data for the model ---\n",
    "# Use TransactionEncoder to create a one-hot encoded DataFrame from all transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(all_transactions).transform(all_transactions)\n",
    "df_one_hot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(\"\\nOne-hot encoded data head:\")\n",
    "print(df_one_hot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9da6f794-bc0c-4977-9054-933532cff5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation process starting with a sample of transactions...\n",
      "\n",
      "Model trained and rules generated on the training data.\n",
      "\n",
      "Model Recall@3 on the simulated test set: 0.3333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume the 'all_transactions' list has been successfully created by the previous chunk-based code\n",
    "# For this step, we'll use a placeholder for all_transactions\n",
    "all_transactions = [\n",
    "    ['6 pc grilled wings combo', 'large cheese fries', '20 oz soda'],\n",
    "    ['hot honey rub (boneless)', 'mango habanero (boneless)', 'garlic parmesan (tenders)'],\n",
    "    ['10 pc grilled wings combo', '10 pc grilled wings combo', 'ranch dip - regular'],\n",
    "    ['6 pc boneless mild', 'large cheese fries', '20 oz soda'],\n",
    "    ['spicy boneless wings combo', 'regular buffalo fries', 'blue cheese dip'],\n",
    "    ['hot honey rub (boneless)', 'mango habanero (boneless)', 'garlic parmesan (tenders)'],\n",
    "    ['10 pc grilled wings combo', 'large seasoned fries', 'large seasoned fries']\n",
    "]\n",
    "print(\"Validation process starting with a sample of transactions...\")\n",
    "\n",
    "# --- 1. Split the transactions for validation ---\n",
    "# We'll use 70% for training the rules and 30% for testing their performance\n",
    "train_transactions, test_transactions = train_test_split(all_transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 2. Train the model on the training set ---\n",
    "# Use TransactionEncoder to create a one-hot encoded DataFrame\n",
    "te = TransactionEncoder()\n",
    "te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "# Use the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets_train = apriori(df_one_hot_train, min_support=0.005, use_colnames=True)\n",
    "# Generate association rules\n",
    "rules_train = association_rules(frequent_itemsets_train, metric=\"confidence\", min_threshold=0.5)\n",
    "rules_train.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "print(\"\\nModel trained and rules generated on the training data.\")\n",
    "\n",
    "\n",
    "# --- 3. Define the recommendation and evaluation functions ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    \"\"\"Generates top-k recommendations based on association rules.\"\"\"\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    \"\"\"Calculates Recall@k by simulating the prediction scenario.\"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "\n",
    "    for order in test_transactions:\n",
    "        # We need at least 2 items to simulate a missing item\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "\n",
    "        # Simulate a missing item by hiding the last item\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        \n",
    "        total_queries += 1\n",
    "        \n",
    "        # Get recommendations for the simulated cart\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        \n",
    "        # Check if the missing item is in the top recommendations\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "# --- 4. Validate the model and print the Recall@3 score ---\n",
    "recall_score = calculate_recall_at_k(rules_train, test_transactions)\n",
    "print(f\"\\nModel Recall@3 on the simulated test set: {recall_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5666dffc-8b88-474e-814d-559e800f8461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ITEMS_LIST'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ITEMS_LIST'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     exit()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print the data type of the 'ITEMS_LIST' column\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData type of the ITEMS_LIST column:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITEMS_LIST\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print the first entry of the 'ITEMS_LIST' column\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst entry of the ITEMS_LIST column:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITEMS_LIST\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ITEMS_LIST'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file with all engineered features\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Print the data type of the 'ITEMS_LIST' column\n",
    "print(\"Data type of the ITEMS_LIST column:\", final_dataset['ITEMS_LIST'].dtype)\n",
    "\n",
    "# Print the first entry of the 'ITEMS_LIST' column\n",
    "print(\"First entry of the ITEMS_LIST column:\", final_dataset['ITEMS_LIST'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e3e3ea-a4d0-42fa-a197-1469e3122037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of the ITEMS_LIST column: object\n",
      "First entry of the ITEMS_LIST column: ['10 pc Grilled Wings Combo', '8 pc Grilled Wings Combo', '8 pc Spicy Wings Combo']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file with all engineered features\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Identify all columns that contain 'item' and 'name'\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "\n",
    "# Create a new column 'ITEMS_LIST' which contains a list of all items in each order\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Now, perform the check\n",
    "print(\"Data type of the ITEMS_LIST column:\", final_dataset['ITEMS_LIST'].dtype)\n",
    "print(\"First entry of the ITEMS_LIST column:\", final_dataset['ITEMS_LIST'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d17c097-51a3-4b22-95f5-2b7b450232fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- Step 1: Data Preparation ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the file with all engineered features\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     final_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_dataset_with_all_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_dataset_with_all_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation ---\n",
    "# Load the file with all engineered features\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "transactions = final_dataset_filtered['ITEMS_LIST'].tolist()\n",
    "\n",
    "print(f\"Total orders for validation: {len(transactions)}\")\n",
    "\n",
    "# --- Step 2: Train/Test Split ---\n",
    "# We'll use 70% for training the rules and 30% for testing their performance\n",
    "train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Step 3: Train the model on the training set ---\n",
    "te = TransactionEncoder()\n",
    "te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets_train = apriori(df_one_hot_train, min_support=0.003, use_colnames=True)\n",
    "rules_train = association_rules(frequent_itemsets_train, metric=\"confidence\", min_threshold=0.4)\n",
    "rules_train.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "print(\"\\nModel trained and rules generated on the training data.\")\n",
    "\n",
    "# --- Step 4: Define the recommendation and evaluation functions ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    \"\"\"Generates top-k recommendations based on association rules.\"\"\"\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    \"\"\"Calculates Recall@k by simulating the prediction scenario.\"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "\n",
    "    for order in test_transactions:\n",
    "        # We need at least 2 items to simulate a missing item\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Simulate a missing item by hiding the last item\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        \n",
    "        total_queries += 1\n",
    "        \n",
    "        # Get recommendations for the simulated cart\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        \n",
    "        # Check if the missing item is in the top recommendations\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "# --- Step 5: Validate the model and print the Recall@3 score ---\n",
    "recall_score = calculate_recall_at_k(rules_train, test_transactions)\n",
    "print(f\"\\nModel Recall@3 on the simulated test set: {recall_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a961074-23f7-4a9d-bebd-4fa5df00a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total orders for validation: 1411675\n",
      "Data split into training and testing sets.\n",
      "\n",
      "Model trained and rules generated with updated memory-efficient parameters.\n",
      "\n",
      "Model Recall@3 on the simulated test set: 0.1376\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation with Chunking ---\n",
    "file_path = 'final_dataset_with_all_features.csv'\n",
    "chunk_size = 10000  # Adjust this size based on your system's memory\n",
    "all_transactions = []\n",
    "item_name_columns = None\n",
    "\n",
    "try:\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):\n",
    "        if item_name_columns is None:\n",
    "            item_name_columns = [col for col in chunk.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "        \n",
    "        chunk['ITEMS_LIST'] = chunk[item_name_columns].apply(\n",
    "            lambda row: [item for item in row.dropna()], axis=1\n",
    "        )\n",
    "        \n",
    "        all_transactions.extend(chunk[chunk['ITEMS_LIST'].str.len() > 0]['ITEMS_LIST'].tolist())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Total orders for validation: {len(all_transactions)}\")\n",
    "\n",
    "\n",
    "# --- Step 2: Train/Test Split ---\n",
    "train_transactions, test_transactions = train_test_split(all_transactions, test_size=0.3, random_state=42)\n",
    "print(\"Data split into training and testing sets.\")\n",
    "\n",
    "\n",
    "# --- Step 3: Train the model on the training set with memory fixes ---\n",
    "te = TransactionEncoder()\n",
    "te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets_train = apriori(df_one_hot_train, min_support=0.0005, max_len=4, low_memory=True, use_colnames=True)\n",
    "rules_train = association_rules(frequent_itemsets_train, metric=\"confidence\", min_threshold=0.4)\n",
    "rules_train.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "print(\"\\nModel trained and rules generated with updated memory-efficient parameters.\")\n",
    "\n",
    "\n",
    "# --- Step 4: Define the recommendation and evaluation functions ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "# --- Step 5: Validate the model and print the Recall@3 score ---\n",
    "recall_score = calculate_recall_at_k(rules_train, test_transactions)\n",
    "print(f\"\\nModel Recall@3 on the simulated test set: {recall_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d68aa24a-6283-4281-8b37-59fe70e48c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (0), Guest (0), Special Membership (0)\n",
      "\n",
      "--- Training for Registered segment ---\n",
      "Not enough data to train. Skipping this segment.\n",
      "\n",
      "--- Training for Guest segment ---\n",
      "Not enough data to train. Skipping this segment.\n",
      "\n",
      "--- Training for Special Membership segment ---\n",
      "Not enough data to train. Skipping this segment.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Segment the data by 'CUSTOMER_TYPE'\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "special_membership_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'special membership']\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), Special Membership ({len(special_membership_data)})\")\n",
    "\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support=0.0005, min_confidence=0.4):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None\n",
    "\n",
    "    # Create transactions from the segment\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train Apriori model\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    # Validate the model\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "registered_rules = train_and_validate_segment(registered_data, 'Registered')\n",
    "guest_rules = train_and_validate_segment(guest_data, 'Guest')\n",
    "special_membership_rules = train_and_validate_segment(special_membership_data, 'Special Membership')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55d7f482-739f-4fff-a8e3-12ff81ad5f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'CUSTOMER_TYPE' column and their counts:\n",
      "CUSTOMER_TYPE\n",
      "Registered         1140068\n",
      "Guest               271545\n",
      "eClub                 1904\n",
      "Deleted Account        832\n",
      "Unknown                 59\n",
      "Online                   2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Get the unique values and their counts from the CUSTOMER_TYPE column\n",
    "customer_type_counts = final_dataset['CUSTOMER_TYPE'].value_counts()\n",
    "print(\"Unique values in 'CUSTOMER_TYPE' column and their counts:\")\n",
    "print(customer_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a932c1f9-17b7-4b1b-9415-41c91231c985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009)\n",
      "\n",
      "--- Training for Registered segment ---\n",
      "Generated 174 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.1458\n",
      "\n",
      "--- Training for Guest segment ---\n",
      "Generated 175 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.1383\n",
      "\n",
      "--- Training for eClub segment ---\n",
      "Generated 1530 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.0231\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Segmentation (Corrected) ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by the actual 'CUSTOMER_TYPE' values\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub']\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support=0.0005, min_confidence=0.4):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered')\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest')\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b6322b8-3432-4bd4-ac95-b2019f701781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009)\n",
      "\n",
      "--- Training for Registered segment with min_support=0.0005 ---\n",
      "Generated 174 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.1458\n",
      "\n",
      "--- Training for Guest segment with min_support=0.0005 ---\n",
      "Generated 175 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.1383\n",
      "\n",
      "--- Training for eClub segment with min_support=0.0035 ---\n",
      "Generated 68 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.2211\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Segmentation (Corrected) ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by the actual 'CUSTOMER_TYPE' values\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub']\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment with min_support={min_support} ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment with Hybrid Parameters ---\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=0.0005, min_confidence=0.4)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=0.0005, min_confidence=0.4)\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=0.0035, min_confidence=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a0d3955-848c-49ff-8927-db1e2a9c6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009)\n",
      "\n",
      "--- Training for Registered segment with min_support=0.0003 ---\n",
      "Generated 247 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.1597\n",
      "\n",
      "--- Training for Guest segment with min_support=0.0003 ---\n",
      "Generated 251 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.1623\n",
      "\n",
      "--- Training for eClub segment with min_support=0.0025 ---\n",
      "Generated 229 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.1188\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Segmentation (Corrected) ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by the actual 'CUSTOMER_TYPE' values\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub']\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment with min_support={min_support} ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment with Hybrid Parameters ---\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=0.0003, min_confidence=0.4)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=0.0003, min_confidence=0.4)\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=0.0025, min_confidence=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a933a8b-0b15-4fda-bdfa-676d30fc3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009)\n",
      "\n",
      "--- Training for Registered segment with min_support=0.0003 ---\n",
      "Generated 299 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.1598\n",
      "\n",
      "--- Training for Guest segment with min_support=0.0003 ---\n",
      "Generated 310 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.2095\n",
      "\n",
      "--- Training for eClub segment with min_support=0.0025 ---\n",
      "Generated 271 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.1848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Segmentation (Corrected) ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by the actual 'CUSTOMER_TYPE' values\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub']\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment with min_support={min_support} ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=5, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment with Hybrid Parameters ---\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=0.0003, min_confidence=0.4)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=0.0003, min_confidence=0.4)\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=0.0025, min_confidence=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdc7c266-37ce-4940-bd98-3eb7d34962ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009)\n",
      "\n",
      "--- Training for Registered segment with min_support=0.0002 ---\n",
      "Generated 453 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.2188\n",
      "\n",
      "--- Training for Guest segment with min_support=0.00025 ---\n",
      "Generated 373 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.2154\n",
      "\n",
      "--- Training for eClub segment with min_support=0.0025 ---\n",
      "Generated 271 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.1848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Segmentation (Corrected) ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by the actual 'CUSTOMER_TYPE' values\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub']\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment with min_support={min_support} ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=5, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment with Hybrid Parameters ---\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=0.0002, min_confidence=0.4)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=0.00025, min_confidence=0.4)\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=0.0025, min_confidence=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c615d58-6d9a-4dc9-8bcc-2259b66653d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009)\n",
      "\n",
      "--- Training for Registered segment with min_support=0.0001 ---\n",
      "Generated 910 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.2297\n",
      "\n",
      "--- Training for Guest segment with min_support=0.00015 ---\n",
      "Generated 665 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.2013\n",
      "\n",
      "--- Training for eClub segment with min_support=0.003 ---\n",
      "Generated 68 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.2211\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Segmentation (Corrected) ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by the actual 'CUSTOMER_TYPE' values\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub']\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment with min_support={min_support} ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=5, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment with Hybrid Parameters ---\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=0.0001, min_confidence=0.4)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=0.0002, min_confidence=0.4)\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=0.003, min_confidence=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd8d9a77-e770-4f4a-a502-12852ceeb165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'ORDER_OCCASION_NAME' column:\n",
      "['ToGo' 'Delivery']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Get the unique values from the 'ORDER_OCCASION_NAME' column\n",
    "unique_order_occasions = final_dataset['ORDER_OCCASION_NAME'].unique()\n",
    "print(\"Unique values in 'ORDER_OCCASION_NAME' column:\")\n",
    "print(unique_order_occasions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87928038-f19c-4d20-ab0d-e950c1c64410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered ToGo (562873), Registered Delivery (102217), Guest ToGo (128453), Guest Delivery (17466), eClub ToGo (895), eClub Delivery (114), General (812560)\n",
      "\n",
      "--- Training for Registered ToGo segment ---\n",
      "Generated 445 rules for Registered ToGo.\n",
      "Validation complete. Recall@3 for Registered ToGo is: 0.1829\n",
      "\n",
      "--- Training for Registered Delivery segment ---\n",
      "Generated 635 rules for Registered Delivery.\n",
      "Validation complete. Recall@3 for Registered Delivery is: 0.2047\n",
      "\n",
      "--- Training for Guest ToGo segment ---\n",
      "Generated 509 rules for Guest ToGo.\n",
      "Validation complete. Recall@3 for Guest ToGo is: 0.2021\n",
      "\n",
      "--- Training for Guest Delivery segment ---\n",
      "Generated 1173 rules for Guest Delivery.\n",
      "Validation complete. Recall@3 for Guest Delivery is: 0.0853\n",
      "\n",
      "--- Training for eClub ToGo segment ---\n",
      "Generated 1523 rules for eClub ToGo.\n",
      "Validation complete. Recall@3 for eClub ToGo is: 0.0446\n",
      "\n",
      "--- Training for eClub Delivery segment ---\n",
      "Generated 558 rules for eClub Delivery.\n",
      "Validation complete. Recall@3 for eClub Delivery is: 0.0000\n",
      "\n",
      "--- Training for General Fallback segment ---\n",
      "Generated 456 rules for General Fallback.\n",
      "Validation complete. Recall@3 for General Fallback is: 0.1962\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Nested Segmentation (Corrected) ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column for training\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by 'CUSTOMER_TYPE' and 'ORDER_OCCASION_NAME'\n",
    "registered_togo_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered') & (final_dataset_filtered['ORDER_OCCASION_NAME'] == 'ToGo')]\n",
    "registered_delivery_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'Registered') & (final_dataset_filtered['ORDER_OCCASION_NAME'] == 'Delivery')]\n",
    "\n",
    "guest_togo_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest') & (final_dataset_filtered['ORDER_OCCASION_NAME'] == 'ToGo')]\n",
    "guest_delivery_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'Guest') & (final_dataset_filtered['ORDER_OCCASION_NAME'] == 'Delivery')]\n",
    "\n",
    "eclub_togo_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub') & (final_dataset_filtered['ORDER_OCCASION_NAME'] == 'ToGo')]\n",
    "eclub_delivery_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'eClub') & (final_dataset_filtered['ORDER_OCCASION_NAME'] == 'Delivery')]\n",
    "\n",
    "general_data = final_dataset_filtered.copy()\n",
    "\n",
    "print(f\"Data segmented into: Registered ToGo ({len(registered_togo_data)}), Registered Delivery ({len(registered_delivery_data)}), Guest ToGo ({len(guest_togo_data)}), Guest Delivery ({len(guest_delivery_data)}), eClub ToGo ({len(eclub_togo_data)}), eClub Delivery ({len(eclub_delivery_data)}), General ({len(general_data)})\")\n",
    "\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.0003\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "# Segmented models for Registered customers\n",
    "reg_togo_rules, reg_togo_score = train_and_validate_segment(registered_togo_data, 'Registered ToGo', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "reg_del_rules, reg_del_score = train_and_validate_segment(registered_delivery_data, 'Registered Delivery', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "# Segmented models for Guest customers\n",
    "guest_togo_rules, guest_togo_score = train_and_validate_segment(guest_togo_data, 'Guest ToGo', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_del_rules, guest_del_score = train_and_validate_segment(guest_delivery_data, 'Guest Delivery', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "# Segmented models for eClub customers\n",
    "eclub_togo_rules, eclub_togo_score = train_and_validate_segment(eclub_togo_data, 'eClub ToGo', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "eclub_del_rules, eclub_del_score = train_and_validate_segment(eclub_delivery_data, 'eClub Delivery', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "\n",
    "# Fallback model\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_large, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9c2cbc9-374c-45b1-9fce-0ab0f08e880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009), General (812560)\n",
      "\n",
      "--- Training for Registered segment with min_support=0.0001 ---\n",
      "Generated 464 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.1994\n",
      "\n",
      "--- Training for Guest segment with min_support=0.00015 ---\n",
      "Generated 381 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.1918\n",
      "\n",
      "--- Training for eClub segment with min_support=0.003 ---\n",
      "Generated 68 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.2211\n",
      "\n",
      "--- Training for General Fallback segment with min_support=0.0001 ---\n",
      "Generated 456 rules for General Fallback.\n",
      "Validation complete. Recall@3 for General Fallback is: 0.1962\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation and Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean customer type for consistency\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by 'CUSTOMER_TYPE'\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eclub']\n",
    "\n",
    "# General fallback data includes all orders\n",
    "general_data = final_dataset_filtered.copy()\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)}), General ({len(general_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment with min_support={min_support} ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "# Use the optimal parameters we found through our previous analysis\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.0035\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=0.0001, min_confidence=min_confidence_val)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=0.00015, min_confidence=min_confidence_val)\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=0.003, min_confidence=min_confidence_val)\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_large, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61f5fb98-eb75-430d-ac7f-60ad12b5c3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), eClub (1009), General Fallback (542)\n",
      "\n",
      "--- Training for Registered segment ---\n",
      "Generated 464 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.1994\n",
      "\n",
      "--- Training for Guest segment ---\n",
      "Generated 381 rules for Guest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m min_confidence_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[0;32m     92\u001b[0m registered_rules, registered_score \u001b[38;5;241m=\u001b[39m train_and_validate_segment(registered_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegistered\u001b[39m\u001b[38;5;124m'\u001b[39m, min_support\u001b[38;5;241m=\u001b[39mmin_support_large, min_confidence\u001b[38;5;241m=\u001b[39mmin_confidence_val)\n\u001b[1;32m---> 93\u001b[0m guest_rules, guest_score \u001b[38;5;241m=\u001b[39m train_and_validate_segment(guest_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuest\u001b[39m\u001b[38;5;124m'\u001b[39m, min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00015\u001b[39m, min_confidence\u001b[38;5;241m=\u001b[39mmin_confidence_val)\n\u001b[0;32m     94\u001b[0m eclub_rules, eclub_score \u001b[38;5;241m=\u001b[39m train_and_validate_segment(eclub_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meClub\u001b[39m\u001b[38;5;124m'\u001b[39m, min_support\u001b[38;5;241m=\u001b[39mmin_support_small, min_confidence\u001b[38;5;241m=\u001b[39mmin_confidence_val)\n\u001b[0;32m     95\u001b[0m general_rules, general_score \u001b[38;5;241m=\u001b[39m train_and_validate_segment(general_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneral Fallback\u001b[39m\u001b[38;5;124m'\u001b[39m, min_support\u001b[38;5;241m=\u001b[39mmin_support_small, min_confidence\u001b[38;5;241m=\u001b[39mmin_confidence_val)\n",
      "Cell \u001b[1;32mIn[76], line 83\u001b[0m, in \u001b[0;36mtrain_and_validate_segment\u001b[1;34m(segment_df, segment_name, min_support, min_confidence)\u001b[0m\n\u001b[0;32m     79\u001b[0m rules\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlift\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rules for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m recall_score \u001b[38;5;241m=\u001b[39m calculate_recall_at_k(rules, test_transactions)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation complete. Recall@3 for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rules, recall_score\n",
      "Cell \u001b[1;32mIn[76], line 58\u001b[0m, in \u001b[0;36mcalculate_recall_at_k\u001b[1;34m(rules, test_transactions, k)\u001b[0m\n\u001b[0;32m     56\u001b[0m simulated_cart \u001b[38;5;241m=\u001b[39m order[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     57\u001b[0m total_queries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 58\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_recommendations(simulated_cart, rules, k)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_item \u001b[38;5;129;01min\u001b[39;00m recommendations:\n\u001b[0;32m     60\u001b[0m     correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[76], line 38\u001b[0m, in \u001b[0;36mget_recommendations\u001b[1;34m(cart_items, rules_df, k)\u001b[0m\n\u001b[0;32m     36\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m cart_items:\n\u001b[1;32m---> 38\u001b[0m     matching_rules \u001b[38;5;241m=\u001b[39m rules_df[rules_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mantecedents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: item \u001b[38;5;129;01min\u001b[39;00m x)]\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matching_rules\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _, rule_row \u001b[38;5;129;01min\u001b[39;00m matching_rules\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4093\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   4092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 4093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_bool_array(key)\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   4097\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4155\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4154\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 4155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation and Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean customer type for consistency\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by 'CUSTOMER_TYPE'\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eclub']\n",
    "\n",
    "# General fallback data now includes only the remaining rows\n",
    "main_types = ['registered', 'guest', 'eclub']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), eClub ({len(eclub_data)}), General Fallback ({len(general_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.003\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=0.00015, min_confidence=min_confidence_val)\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_small, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32ade840-f500-4238-9913-19b64967afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Reg. ToGo (562873), Reg. Delivery (102217), Guest ToGo (128453), Guest Delivery (17466), eClub (1009), General Fallback (542)\n",
      "\n",
      "--- Training for Registered ToGo segment ---\n",
      "Generated 837 rules for Registered ToGo.\n",
      "Validation complete. Recall@3 for Registered ToGo is: 0.2011\n",
      "\n",
      "--- Training for Registered Delivery segment ---\n",
      "Generated 1915 rules for Registered Delivery.\n",
      "Validation complete. Recall@3 for Registered Delivery is: 0.1511\n",
      "\n",
      "--- Training for Guest ToGo segment ---\n",
      "Generated 1830 rules for Guest ToGo.\n",
      "Validation complete. Recall@3 for Guest ToGo is: 0.1880\n",
      "\n",
      "--- Training for Guest Delivery segment ---\n",
      "Generated 7289 rules for Guest Delivery.\n",
      "Validation complete. Recall@3 for Guest Delivery is: 0.1246\n",
      "\n",
      "--- Training for eClub segment ---\n",
      "Generated 68 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.2211\n",
      "\n",
      "--- Training for General Fallback segment ---\n",
      "Generated 112 rules for General Fallback.\n",
      "Validation complete. Recall@3 for General Fallback is: 0.1963\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation and Nested Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean customer/order data\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "final_dataset['ORDER_OCCASION_NAME'] = final_dataset['ORDER_OCCASION_NAME'].str.lower()\n",
    "\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Segment the data\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eclub']\n",
    "\n",
    "# Nested segments for Registered customers\n",
    "reg_togo_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "reg_delivery_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "# Nested segments for Guest customers\n",
    "guest_togo_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "guest_delivery_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "# Fallback data\n",
    "main_types = ['registered', 'guest', 'eclub']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "print(f\"Data segmented into: Reg. ToGo ({len(reg_togo_data)}), Reg. Delivery ({len(reg_delivery_data)}), Guest ToGo ({len(guest_togo_data)}), Guest Delivery ({len(guest_delivery_data)}), eClub ({len(eclub_data)}), General Fallback ({len(general_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=6, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.003\n",
    "min_confidence_val = 0.5\n",
    "\n",
    "# Segmented models for Registered customers\n",
    "reg_togo_rules, reg_togo_score = train_and_validate_segment(reg_togo_data, 'Registered ToGo', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "reg_del_rules, reg_del_score = train_and_validate_segment(reg_delivery_data, 'Registered Delivery', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "# Segmented models for Guest customers\n",
    "guest_togo_rules, guest_togo_score = train_and_validate_segment(guest_togo_data, 'Guest ToGo', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_del_rules, guest_del_score = train_and_validate_segment(guest_delivery_data, 'Guest Delivery', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "# Single model for eClub and General Fallback\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_small, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "114a4e52-7c64-401e-b007-fa7ce8faf611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Reg. ToGo (562873), Reg. Delivery (102217), Guest ToGo (128453), Guest Delivery (17466), eClub (1009), General Fallback (542)\n",
      "\n",
      "--- Training for Registered ToGo segment ---\n",
      "Generated 837 rules for Registered ToGo.\n",
      "Validation complete. Recall@3 for Registered ToGo is: 0.2011\n",
      "\n",
      "--- Training for Registered Delivery segment ---\n",
      "Generated 783 rules for Registered Delivery.\n",
      "Validation complete. Recall@3 for Registered Delivery is: 0.1758\n",
      "\n",
      "--- Training for Guest ToGo segment ---\n",
      "Generated 535 rules for Guest ToGo.\n",
      "Validation complete. Recall@3 for Guest ToGo is: 0.2085\n",
      "\n",
      "--- Training for Guest Delivery segment ---\n",
      "Generated 1798 rules for Guest Delivery.\n",
      "Validation complete. Recall@3 for Guest Delivery is: 0.1557\n",
      "\n",
      "--- Training for eClub segment ---\n",
      "Generated 68 rules for eClub.\n",
      "Validation complete. Recall@3 for eClub is: 0.2211\n",
      "\n",
      "--- Training for General Fallback segment ---\n",
      "Generated 112 rules for General Fallback.\n",
      "Validation complete. Recall@3 for General Fallback is: 0.1963\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation and Nested Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean customer/order data\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "final_dataset['ORDER_OCCASION_NAME'] = final_dataset['ORDER_OCCASION_NAME'].str.lower()\n",
    "\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Segment the data\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "eclub_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'eclub']\n",
    "\n",
    "# Nested segments for Registered customers\n",
    "reg_togo_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "reg_delivery_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "# Nested segments for Guest customers\n",
    "guest_togo_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "guest_delivery_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "# Fallback data\n",
    "main_types = ['registered', 'guest', 'eclub']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "print(f\"Data segmented into: Reg. ToGo ({len(reg_togo_data)}), Reg. Delivery ({len(reg_delivery_data)}), Guest ToGo ({len(guest_togo_data)}), Guest Delivery ({len(guest_delivery_data)}), eClub ({len(eclub_data)}), General Fallback ({len(general_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=6, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "min_support_large = 0.0002\n",
    "min_support_small = 0.003\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "# Segmented models for Registered customers\n",
    "reg_togo_rules, reg_togo_score = train_and_validate_segment(reg_togo_data, 'Registered ToGo', min_support=0.0001, min_confidence=min_confidence_val)\n",
    "reg_del_rules, reg_del_score = train_and_validate_segment(reg_delivery_data, 'Registered Delivery', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "# Segmented models for Guest customers\n",
    "guest_togo_rules, guest_togo_score = train_and_validate_segment(guest_togo_data, 'Guest ToGo', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_del_rules, guest_del_score = train_and_validate_segment(guest_delivery_data, 'Guest Delivery', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "# Single model for eClub and General Fallback\n",
    "eclub_rules, eclub_score = train_and_validate_segment(eclub_data, 'eClub', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_small, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea39d810-97c1-4257-a1a5-914af97305b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Reg. Weekday (319214), Reg. Weekend (345876), Guest Weekday (68691), Guest Weekend (77228), eClub Weekday (483), eClub Weekend (526), General Fallback (542)\n",
      "\n",
      "--- Training for Registered Weekday segment ---\n",
      "Generated 478 rules for Registered Weekday.\n",
      "Validation complete. Recall@3 for Registered Weekday is: 0.2098\n",
      "\n",
      "--- Training for Registered Weekend segment ---\n",
      "Generated 453 rules for Registered Weekend.\n",
      "Validation complete. Recall@3 for Registered Weekend is: 0.1892\n",
      "\n",
      "--- Training for Guest Weekday segment ---\n",
      "Generated 566 rules for Guest Weekday.\n",
      "Validation complete. Recall@3 for Guest Weekday is: 0.2232\n",
      "\n",
      "--- Training for Guest Weekend segment ---\n",
      "Generated 548 rules for Guest Weekend.\n",
      "Validation complete. Recall@3 for Guest Weekend is: 0.1917\n",
      "\n",
      "--- Training for eClub Weekday segment ---\n",
      "Generated 141 rules for eClub Weekday.\n",
      "Validation complete. Recall@3 for eClub Weekday is: 0.0828\n",
      "\n",
      "--- Training for eClub Weekend segment ---\n",
      "Generated 101 rules for eClub Weekend.\n",
      "Validation complete. Recall@3 for eClub Weekend is: 0.1329\n",
      "\n",
      "--- Training for General Fallback segment ---\n",
      "Generated 88 rules for General Fallback.\n",
      "Validation complete. Recall@3 for General Fallback is: 0.1595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Feature Creation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate 'ITEMS_LIST' and clean relevant columns\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "final_dataset['ORDER_CREATED_DATE'] = pd.to_datetime(final_dataset['ORDER_CREATED_DATE'])\n",
    "\n",
    "# Create the new 'is_weekend' feature\n",
    "weekend_days = ['Friday', 'Saturday', 'Sunday']\n",
    "final_dataset['order_day_of_week'] = final_dataset['ORDER_CREATED_DATE'].dt.day_name()\n",
    "final_dataset['is_weekend'] = final_dataset['order_day_of_week'].isin(weekend_days).astype(int)\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "\n",
    "# --- Step 2: Nested Segmentation ---\n",
    "registered_weekday_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'registered') & (final_dataset_filtered['is_weekend'] == 0)]\n",
    "registered_weekend_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'registered') & (final_dataset_filtered['is_weekend'] == 1)]\n",
    "\n",
    "guest_weekday_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'guest') & (final_dataset_filtered['is_weekend'] == 0)]\n",
    "guest_weekend_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'guest') & (final_dataset_filtered['is_weekend'] == 1)]\n",
    "\n",
    "eclub_weekday_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'eclub') & (final_dataset_filtered['is_weekend'] == 0)]\n",
    "eclub_weekend_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'eclub') & (final_dataset_filtered['is_weekend'] == 1)]\n",
    "\n",
    "main_types = ['registered', 'guest', 'eclub']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "print(f\"Data segmented into: Reg. Weekday ({len(registered_weekday_data)}), Reg. Weekend ({len(registered_weekend_data)}), Guest Weekday ({len(guest_weekday_data)}), Guest Weekend ({len(guest_weekend_data)}), eClub Weekday ({len(eclub_weekday_data)}), eClub Weekend ({len(eclub_weekend_data)}), General Fallback ({len(general_data)})\")\n",
    "\n",
    "# --- Step 3: Helper Functions and Training/Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 4: Run Training and Validation for Each Segment ---\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.0035\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "reg_weekday_rules, reg_weekday_score = train_and_validate_segment(registered_weekday_data, 'Registered Weekday', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "reg_weekend_rules, reg_weekend_score = train_and_validate_segment(registered_weekend_data, 'Registered Weekend', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "guest_weekday_rules, guest_weekday_score = train_and_validate_segment(guest_weekday_data, 'Guest Weekday', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_weekend_rules, guest_weekend_score = train_and_validate_segment(guest_weekend_data, 'Guest Weekend', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "eclub_weekday_rules, eclub_weekday_score = train_and_validate_segment(eclub_weekday_data, 'eClub Weekday', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "eclub_weekend_rules, eclub_weekend_score = train_and_validate_segment(eclub_weekend_data, 'eClub Weekend', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_small, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa2fa88-21fd-441f-95b8-639ccdfc2534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Association Rules for Registered Weekday ---\n",
      "                                          antecedents  \\\n",
      "9                               (20 pc Crispy Strips)   \n",
      "22                                (50 pc Spicy Wings)   \n",
      "21                              (50 pc Grilled Wings)   \n",
      "197        (Large Buffalo Fries, 30 pc Grilled Wings)   \n",
      "18                                (30 pc Mixed Wings)   \n",
      "239  (Large Veggie Sticks Spicy, Large Buffalo Fries)   \n",
      "16                              (30 pc Grilled Wings)   \n",
      "170                          (20 Oz Soda, 32 Oz Soda)   \n",
      "106         (10 pc Spicy Wings, Cheese Dip - Regular)   \n",
      "28                           (Legendary Feast Bundle)   \n",
      "\n",
      "                 consequents   support  confidence      lift  \n",
      "9      (Large Buffalo Fries)  0.000492    0.460251  6.836123  \n",
      "22       (Ranch Dip - Large)  0.000474    0.546392  5.824382  \n",
      "21       (Ranch Dip - Large)  0.001002    0.507937  5.414460  \n",
      "197      (Ranch Dip - Large)  0.000882    0.460280  4.906459  \n",
      "18       (Ranch Dip - Large)  0.000546    0.426573  4.547152  \n",
      "239      (Ranch Dip - Large)  0.000721    0.409669  4.366958  \n",
      "16       (Ranch Dip - Large)  0.003701    0.400678  4.271117  \n",
      "170  (20pc Spicy Feast Deal)  0.000546    0.514768  3.470758  \n",
      "106  (Regular Buffalo Fries)  0.000685    0.444767  3.363779  \n",
      "28   (Regular Buffalo Fries)  0.002551    0.400844  3.031585  \n",
      "--------------------------------------------------\n",
      "\n",
      "--- Top 10 Association Rules for Registered Weekend ---\n",
      "                                     antecedents              consequents  \\\n",
      "8                          (20 pc Crispy Strips)    (Large Buffalo Fries)   \n",
      "21                         (75 pc Grilled Wings)      (Ranch Dip - Large)   \n",
      "3                         (100 pc Grilled Wings)      (Ranch Dip - Large)   \n",
      "19                           (50 pc Spicy Wings)      (Ranch Dip - Large)   \n",
      "162     (20 pc Grilled Wings, Add 5 Spicy Wings)    (Large Buffalo Fries)   \n",
      "17                         (50 pc Grilled Wings)      (Ranch Dip - Large)   \n",
      "193   (50 pc Grilled Wings, Large Buffalo Fries)      (Ranch Dip - Large)   \n",
      "164  (20 pc Grilled Wings, Cheese Dip - Regular)  (Regular Buffalo Fries)   \n",
      "18                           (50 pc Mixed Wings)      (Ranch Dip - Large)   \n",
      "186   (Large Buffalo Fries, 30 pc Grilled Wings)      (Ranch Dip - Large)   \n",
      "\n",
      "      support  confidence      lift  \n",
      "8    0.000723    0.466667  6.141882  \n",
      "21   0.000570    0.616071  5.824245  \n",
      "3    0.000376    0.590909  5.586364  \n",
      "19   0.000430    0.581006  5.492737  \n",
      "162  0.000430    0.401544  5.284797  \n",
      "17   0.002016    0.522484  4.939483  \n",
      "193  0.000376    0.514124  4.860452  \n",
      "164  0.000302    0.536765  4.775224  \n",
      "18   0.000467    0.495614  4.685459  \n",
      "186  0.001342    0.425950  4.026868  \n",
      "--------------------------------------------------\n",
      "\n",
      "--- Top 10 Association Rules for Guest Weekday ---\n",
      "                                          antecedents  \\\n",
      "14                              (50 pc Grilled Wings)   \n",
      "16                              (75 pc Grilled Wings)   \n",
      "38    (Blue Cheese Dip - Regular, 4 pc Crispy Strips)   \n",
      "239       (Voodoo Fries - Large, Large Buffalo Fries)   \n",
      "176  (20 pc Grilled Wings, Large Veggie Sticks Spicy)   \n",
      "186                       (32 Oz Soda, Dipping Sauce)   \n",
      "162                          (20 Oz Soda, 32 Oz Soda)   \n",
      "98      (Ranch Dip - Regular, Cheese Fries - Regular)   \n",
      "125         (4 pc Crispy Strips, 15 pc Grilled Wings)   \n",
      "97          (10 pc Spicy Wings, Cheese Dip - Regular)   \n",
      "\n",
      "                 consequents   support  confidence      lift  \n",
      "14       (Ranch Dip - Large)  0.001477    0.518248  5.974329  \n",
      "16       (Ranch Dip - Large)  0.000395    0.513514  5.919748  \n",
      "38     (10 pc Grilled Wings)  0.000437    0.428571  5.471853  \n",
      "239      (Ranch Dip - Large)  0.000395    0.431818  4.977970  \n",
      "176      (Ranch Dip - Large)  0.000395    0.413043  4.761537  \n",
      "186  (20pc Spicy Feast Deal)  0.000416    0.540541  3.887930  \n",
      "162  (20pc Spicy Feast Deal)  0.000458    0.536585  3.859482  \n",
      "98       (10 pc Spicy Wings)  0.001643    0.415789  3.755147  \n",
      "125  (Regular Buffalo Fries)  0.000374    0.473684  3.630824  \n",
      "97   (Regular Buffalo Fries)  0.000562    0.409091  3.135711  \n",
      "--------------------------------------------------\n",
      "\n",
      "--- Top 10 Association Rules for Guest Weekend ---\n",
      "                                          antecedents            consequents  \\\n",
      "4                                (100 pc Mixed Wings)    (Ranch Dip - Large)   \n",
      "15                              (30 pc Crispy Strips)  (Large Buffalo Fries)   \n",
      "3                              (100 pc Grilled Wings)    (Ranch Dip - Large)   \n",
      "9                               (20 pc Crispy Strips)  (Large Buffalo Fries)   \n",
      "20                                (50 pc Mixed Wings)    (Ranch Dip - Large)   \n",
      "211  (Large Veggie Sticks Spicy, 30 pc Grilled Wings)    (Ranch Dip - Large)   \n",
      "40         (10 pc Grilled Wings, 30 pc Grilled Wings)  (Large Buffalo Fries)   \n",
      "21                                (50 pc Spicy Wings)    (Ranch Dip - Large)   \n",
      "19                              (50 pc Grilled Wings)    (Ranch Dip - Large)   \n",
      "23                              (75 pc Grilled Wings)    (Ranch Dip - Large)   \n",
      "\n",
      "      support  confidence      lift  \n",
      "4    0.000351    0.633333  6.428345  \n",
      "15   0.000481    0.464286  5.983033  \n",
      "3    0.000795    0.589041  5.978778  \n",
      "9    0.000814    0.463158  5.968499  \n",
      "20   0.000888    0.558140  5.665127  \n",
      "211  0.000388    0.525000  5.328760  \n",
      "40   0.000351    0.413043  5.322698  \n",
      "21   0.000777    0.518519  5.262973  \n",
      "19   0.003015    0.506211  5.138053  \n",
      "23   0.000925    0.500000  5.075009  \n",
      "--------------------------------------------------\n",
      "\n",
      "--- Top 10 Association Rules for eClub Weekday ---\n",
      "                                        antecedents  \\\n",
      "39                (10 pc Grilled Wings, 32 Oz Soda)   \n",
      "109               (Ranch Dip - Regular, 32 Oz Soda)   \n",
      "40                            (Cheese Dip - Medium)   \n",
      "111                           (Cheese Dip - Medium)   \n",
      "112  (6 pc Grilled Wings Combo, Fried Corn - Large)   \n",
      "37       (Cheese Dip - Medium, 10 pc Grilled Wings)   \n",
      "108      (Ranch Dip - Regular, Cheese Dip - Medium)   \n",
      "99     (20pc Spicy Feast Deal, Large Buffalo Fries)   \n",
      "3                         (10 pc Mixed Wings Combo)   \n",
      "71     (6 pc Spicy Wings Combo, 2 pc Crispy Strips)   \n",
      "\n",
      "                           consequents   support  confidence       lift  \n",
      "39               (Cheese Dip - Medium)  0.005917    0.666667  56.333333  \n",
      "109              (Cheese Dip - Medium)  0.005917    0.666667  56.333333  \n",
      "40   (10 pc Grilled Wings, 32 Oz Soda)  0.005917    0.500000  56.333333  \n",
      "111  (Ranch Dip - Regular, 32 Oz Soda)  0.005917    0.500000  56.333333  \n",
      "112         (5 pc Crispy Strips Combo)  0.005917    0.666667  32.190476  \n",
      "37                        (32 Oz Soda)  0.005917    1.000000  30.727273  \n",
      "108                       (32 Oz Soda)  0.005917    1.000000  30.727273  \n",
      "99         (Large Veggie Sticks Spicy)  0.005917    0.500000  28.166667  \n",
      "3            (10 pc Spicy Wings Combo)  0.005917    0.666667  25.037037  \n",
      "71           (10 pc Spicy Wings Combo)  0.005917    0.666667  25.037037  \n",
      "--------------------------------------------------\n",
      "\n",
      "--- Top 10 Association Rules for eClub Weekend ---\n",
      "                                          antecedents  \\\n",
      "6                             (Triple Chocolate Cake)   \n",
      "86            (Chicken Sub, 8 pc Grilled Wings Combo)   \n",
      "23                               (7 pc Crispy Strips)   \n",
      "92     (Large Veggie Sticks Spicy, Add 5 Spicy Wings)   \n",
      "75      (3 pc Crispy Strips Combo, Ranch Dip - Large)   \n",
      "65       (Regular Buffalo Fries, Large Buffalo Fries)   \n",
      "72  (6 pc Grilled Wings Combo, 3 pc Crispy Strips ...   \n",
      "95             (Ranch Dip - Large, Add 5 Spicy Wings)   \n",
      "93                (Flavor Platter, Add 5 Spicy Wings)   \n",
      "91        (Flavor Platter, Large Veggie Sticks Spicy)   \n",
      "\n",
      "                    consequents   support  confidence       lift  \n",
      "6           (10 pc Spicy Wings)  0.005435    1.000000  40.888889  \n",
      "86  (Large Veggie Sticks Spicy)  0.005435    1.000000  28.307692  \n",
      "23     (Voodoo Fries - Regular)  0.005435    0.400000  24.533333  \n",
      "92             (Flavor Platter)  0.005435    1.000000  23.000000  \n",
      "75        (Cheese Dip - Medium)  0.005435    0.500000  18.400000  \n",
      "65        (20 pc Grilled Wings)  0.005435    1.000000  17.523810  \n",
      "72     (6 pc Spicy Wings Combo)  0.005435    0.666667  17.523810  \n",
      "95             (Flavor Platter)  0.005435    0.666667  15.333333  \n",
      "93  (Large Veggie Sticks Spicy)  0.005435    0.500000  14.153846  \n",
      "91          (Add 5 Spicy Wings)  0.005435    1.000000  13.629630  \n",
      "--------------------------------------------------\n",
      "\n",
      "--- Top 10 Association Rules for General Fallback ---\n",
      "                                          antecedents  \\\n",
      "84         (Ranch Dip - Regular, Cheese Dip - Medium)   \n",
      "54      (Regular Buffalo Fries, Cheese Dip - Regular)   \n",
      "52         (15 pc Spicy Wings, Regular Buffalo Fries)   \n",
      "33          (10 pc Spicy Wings, Fried Corn - Regular)   \n",
      "44          (2 pc Crispy Strips, 15 pc Grilled Wings)   \n",
      "48           (15 pc Grilled Wings, Ranch Dip - Large)   \n",
      "14                           (Cheese Fries - Regular)   \n",
      "45                   (2 pc Crispy Strips, 32 Oz Soda)   \n",
      "65  (Voodoo Fries - Regular, 6 pc Grilled Wings Co...   \n",
      "34                    (10 pc Spicy Wings, 32 Oz Soda)   \n",
      "\n",
      "                   consequents   support  confidence       lift  \n",
      "84             (Veggie Sticks)  0.005277    1.000000  47.375000  \n",
      "54         (15 pc Spicy Wings)  0.005277    0.666667  36.095238  \n",
      "52      (Cheese Dip - Regular)  0.005277    0.666667  31.583333  \n",
      "33                (32 Oz Soda)  0.005277    1.000000  23.687500  \n",
      "44                (32 Oz Soda)  0.005277    1.000000  23.687500  \n",
      "48                (32 Oz Soda)  0.005277    1.000000  23.687500  \n",
      "14        (4 pc Crispy Strips)  0.005277    1.000000  22.294118  \n",
      "45       (15 pc Grilled Wings)  0.005277    0.666667  21.055556  \n",
      "65  (3 pc Crispy Strips Combo)  0.005277    0.666667  19.435897  \n",
      "34      (Fried Corn - Regular)  0.005277    1.000000  18.950000  \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "def display_top_rules(rules_df, segment_name, top_n=10):\n",
    "    \"\"\"Sorts and displays the top N rules for a given segment.\"\"\"\n",
    "    if rules_df is None or rules_df.empty:\n",
    "        print(f\"No rules found for {segment_name}.\")\n",
    "        return\n",
    "\n",
    "    # Sort rules by lift and confidence\n",
    "    best_rules = rules_df.sort_values(by=['lift', 'confidence'], ascending=False).head(top_n)\n",
    "\n",
    "    print(f\"\\n--- Top {top_n} Association Rules for {segment_name} ---\")\n",
    "    print(best_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Assuming you have run the training code and the following rule dataframes are defined:\n",
    "# reg_weekday_rules, reg_weekend_rules, guest_weekday_rules, guest_weekend_rules, eclub_weekday_rules, eclub_weekend_rules, and general_rules\n",
    "\n",
    "# Display the top rules for each segment\n",
    "display_top_rules(reg_weekday_rules, 'Registered Weekday')\n",
    "display_top_rules(reg_weekend_rules, 'Registered Weekend')\n",
    "display_top_rules(guest_weekday_rules, 'Guest Weekday')\n",
    "display_top_rules(guest_weekend_rules, 'Guest Weekend')\n",
    "display_top_rules(eclub_weekday_rules, 'eClub Weekday')\n",
    "display_top_rules(eclub_weekend_rules, 'eClub Weekend')\n",
    "display_top_rules(general_rules, 'General Fallback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9558b4e5-b748-417d-84f1-a2a09d7a6d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All association rules have been saved to 'Top_Association_Rules.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# --- Step 1: Assuming rules DataFrames are already in memory ---\n",
    "# The following variables should be defined from your training run:\n",
    "# reg_weekday_rules, reg_weekend_rules, guest_weekday_rules, guest_weekend_rules,\n",
    "# eclub_weekday_rules, eclub_weekend_rules, general_rules\n",
    "#\n",
    "# If you are getting a NameError, please re-run the full training code first.\n",
    "\n",
    "# --- Step 2: Create a dictionary of rules to export ---\n",
    "rules_to_export = {}\n",
    "\n",
    "rules_to_export['Registered Weekday'] = reg_weekday_rules\n",
    "rules_to_export['Registered Weekend'] = reg_weekend_rules\n",
    "rules_to_export['Guest Weekday'] = guest_weekday_rules\n",
    "rules_to_export['Guest Weekend'] = guest_weekend_rules\n",
    "rules_to_export['eClub Weekday'] = eclub_weekday_rules\n",
    "rules_to_export['eClub Weekend'] = eclub_weekend_rules\n",
    "rules_to_export['General Fallback'] = general_rules\n",
    "\n",
    "\n",
    "# --- Step 3: Export Rules to Excel ---\n",
    "with pd.ExcelWriter('Top_Association_Rules.xlsx') as writer:\n",
    "    for sheet_name, rules_df in rules_to_export.items():\n",
    "        if rules_df is not None and not rules_df.empty:\n",
    "            # Sort the rules by lift and confidence for display\n",
    "            rules_df_sorted = rules_df.sort_values(by=['lift', 'confidence'], ascending=False)\n",
    "            rules_df_sorted.head(10)[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        else:\n",
    "            # Create a simple DataFrame for segments with no rules\n",
    "            no_rules_df = pd.DataFrame([['No rules found for this segment.']], columns=['Info'])\n",
    "            no_rules_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"\\nAll association rules have been saved to 'Top_Association_Rules.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c710b9a0-06de-45f8-9b86-4a3e6127b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.8 MiB for an array with shape (1414410,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- Step 1: Data Preparation and Segmentation ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     final_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_dataset_with_all_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_dataset_with_all_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1066\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1120\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1222\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1833\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.8 MiB for an array with shape (1414410,) and data type int64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation and Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean relevant columns\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "final_dataset['ORDER_CREATED_DATE'] = pd.to_datetime(final_dataset['ORDER_CREATED_DATE'])\n",
    "\n",
    "# Create the new 'is_weekend' feature\n",
    "weekend_days = ['Friday', 'Saturday', 'Sunday']\n",
    "final_dataset['order_day_of_week'] = final_dataset['ORDER_CREATED_DATE'].dt.day_name()\n",
    "final_dataset['is_weekend'] = final_dataset['order_day_of_week'].isin(weekend_days).astype(int)\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Correctly segment the data by 'CUSTOMER_TYPE' and 'is_weekend'\n",
    "registered_weekday_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'registered') & (final_dataset_filtered['is_weekend'] == 0)]\n",
    "registered_weekend_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'registered') & (final_dataset_filtered['is_weekend'] == 1)]\n",
    "guest_weekday_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'guest') & (final_dataset_filtered['is_weekend'] == 0)]\n",
    "guest_weekend_data = final_dataset_filtered[(final_dataset_filtered['CUSTOMER_TYPE'] == 'guest') & (final_dataset_filtered['is_weekend'] == 1)]\n",
    "\n",
    "# General fallback data now includes the remaining rows (eClub, Deleted, Unknown)\n",
    "main_types = ['registered', 'guest']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "print(f\"Data segmented into: Reg. Weekday ({len(registered_weekday_data)}), Reg. Weekend ({len(registered_weekend_data)}), Guest Weekday ({len(guest_weekday_data)}), Guest Weekend ({len(guest_weekend_data)}), General Fallback ({len(general_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.0035\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "reg_weekday_rules, reg_weekday_score = train_and_validate_segment(registered_weekday_data, 'Registered Weekday', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "reg_weekend_rules, reg_weekend_score = train_and_validate_segment(registered_weekend_data, 'Registered Weekend', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_weekday_rules, guest_weekday_score = train_and_validate_segment(guest_weekday_data, 'Guest Weekday', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_weekend_rules, guest_weekend_score = train_and_validate_segment(guest_weekend_data, 'Guest Weekend', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_small, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65137bac-858d-4a6b-bcc7-e0f2306093bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 association rules for each category have been saved to 'Top_Association_Rules.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# --- Step 1: Create a dictionary of rules to export ---\n",
    "# The following variables should be defined from your training run:\n",
    "# reg_weekday_rules, reg_weekend_rules, guest_weekday_rules, guest_weekend_rules,\n",
    "# eclub_weekday_rules, eclub_weekend_rules, general_rules\n",
    "\n",
    "rules_to_export = {}\n",
    "\n",
    "rules_to_export['Registered Weekday'] = reg_weekday_rules\n",
    "rules_to_export['Registered Weekend'] = reg_weekend_rules\n",
    "rules_to_export['Guest Weekday'] = guest_weekday_rules\n",
    "rules_to_export['Guest Weekend'] = guest_weekend_rules\n",
    "rules_to_export['eClub Weekday'] = eclub_weekday_rules\n",
    "rules_to_export['eClub Weekend'] = eclub_weekend_rules\n",
    "rules_to_export['General Fallback'] = general_rules\n",
    "\n",
    "\n",
    "# --- Step 2: Export Rules to Excel ---\n",
    "with pd.ExcelWriter('Top_Association_Rules.xlsx') as writer:\n",
    "    for sheet_name, rules_df in rules_to_export.items():\n",
    "        if rules_df is not None and not rules_df.empty:\n",
    "            # Sort the rules by lift and confidence and take the top 10\n",
    "            rules_df_sorted = rules_df.sort_values(by=['lift', 'confidence'], ascending=False)\n",
    "            rules_df_sorted.head(10)[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        else:\n",
    "            # Create a simple DataFrame for segments with no rules\n",
    "            no_rules_df = pd.DataFrame([['No rules found for this segment.']], columns=['Info'])\n",
    "            no_rules_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"\\nTop 10 association rules for each category have been saved to 'Top_Association_Rules.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc75f12-fe26-4631-9be3-988b2ad3890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Reg. ToGo (562873), Reg. Delivery (102217), Guest ToGo (128453), Guest Delivery (17466), General Fallback (1551)\n",
      "\n",
      "--- Training for Registered ToGo segment ---\n",
      "Generated 445 rules for Registered ToGo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m min_support_small \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0035\u001b[39m\n\u001b[0;32m     99\u001b[0m min_confidence_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m--> 101\u001b[0m reg_togo_rules, reg_togo_score \u001b[38;5;241m=\u001b[39m train_and_validate_segment(reg_togo_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegistered ToGo\u001b[39m\u001b[38;5;124m'\u001b[39m, min_support\u001b[38;5;241m=\u001b[39mmin_support_large, min_confidence\u001b[38;5;241m=\u001b[39mmin_confidence_val)\n\u001b[0;32m    102\u001b[0m reg_del_rules, reg_del_score \u001b[38;5;241m=\u001b[39m train_and_validate_segment(reg_delivery_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegistered Delivery\u001b[39m\u001b[38;5;124m'\u001b[39m, min_support\u001b[38;5;241m=\u001b[39mmin_support_large, min_confidence\u001b[38;5;241m=\u001b[39mmin_confidence_val)\n\u001b[0;32m    103\u001b[0m guest_togo_rules, guest_togo_score \u001b[38;5;241m=\u001b[39m train_and_validate_segment(guest_togo_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuest ToGo\u001b[39m\u001b[38;5;124m'\u001b[39m, min_support\u001b[38;5;241m=\u001b[39mmin_support_large, min_confidence\u001b[38;5;241m=\u001b[39mmin_confidence_val)\n",
      "Cell \u001b[1;32mIn[3], line 92\u001b[0m, in \u001b[0;36mtrain_and_validate_segment\u001b[1;34m(segment_df, segment_name, min_support, min_confidence)\u001b[0m\n\u001b[0;32m     88\u001b[0m rules\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlift\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rules for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m recall_score \u001b[38;5;241m=\u001b[39m calculate_recall_at_k(rules, test_transactions)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation complete. Recall@3 for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rules, recall_score\n",
      "Cell \u001b[1;32mIn[3], line 67\u001b[0m, in \u001b[0;36mcalculate_recall_at_k\u001b[1;34m(rules, test_transactions, k)\u001b[0m\n\u001b[0;32m     65\u001b[0m simulated_cart \u001b[38;5;241m=\u001b[39m order[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     66\u001b[0m total_queries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 67\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_recommendations(simulated_cart, rules, k)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_item \u001b[38;5;129;01min\u001b[39;00m recommendations:\n\u001b[0;32m     69\u001b[0m     correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36mget_recommendations\u001b[1;34m(cart_items, rules_df, k)\u001b[0m\n\u001b[0;32m     45\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m cart_items:\n\u001b[1;32m---> 47\u001b[0m     matching_rules \u001b[38;5;241m=\u001b[39m rules_df[rules_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mantecedents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: item \u001b[38;5;129;01min\u001b[39;00m x)]\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matching_rules\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _, rule_row \u001b[38;5;129;01min\u001b[39;00m matching_rules\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4093\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   4092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 4093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_bool_array(key)\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   4097\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4155\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4154\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 4155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation and Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean relevant columns\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "final_dataset['ORDER_OCCASION_NAME'] = final_dataset['ORDER_OCCASION_NAME'].str.lower()\n",
    "\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Segment the data\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "\n",
    "# Nested segments for Registered customers\n",
    "reg_togo_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "reg_delivery_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "# Nested segments for Guest customers\n",
    "guest_togo_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "guest_delivery_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "# General fallback data now includes the remaining rows (eClub, Deleted, Unknown)\n",
    "main_types = ['registered', 'guest']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "print(f\"Data segmented into: Reg. ToGo ({len(reg_togo_data)}), Reg. Delivery ({len(reg_delivery_data)}), Guest ToGo ({len(guest_togo_data)}), Guest Delivery ({len(guest_delivery_data)}), General Fallback ({len(general_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.0035\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "reg_togo_rules, reg_togo_score = train_and_validate_segment(reg_togo_data, 'Registered ToGo', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "reg_del_rules, reg_del_score = train_and_validate_segment(reg_delivery_data, 'Registered Delivery', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_togo_rules, guest_togo_score = train_and_validate_segment(guest_togo_data, 'Guest ToGo', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_del_rules, guest_del_score = train_and_validate_segment(guest_delivery_data, 'Guest Delivery', min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_small, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce6c7321-5518-42bc-a1c2-d7d5a9ccbceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 association rules for each category have been saved to 'Top_Association_Rules.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# --- Step 1: Create a dictionary of rules to export ---\n",
    "# The following variables should be defined from your training run:\n",
    "# reg_weekday_rules, reg_weekend_rules, guest_weekday_rules, guest_weekend_rules,\n",
    "# general_rules\n",
    "\n",
    "rules_to_export = {}\n",
    "\n",
    "rules_to_export['Registered Weekday'] = reg_weekday_rules\n",
    "rules_to_export['Registered Weekend'] = reg_weekend_rules\n",
    "rules_to_export['Guest Weekday'] = guest_weekday_rules\n",
    "rules_to_export['Guest Weekend'] = guest_weekend_rules\n",
    "rules_to_export['General Fallback'] = general_rules\n",
    "\n",
    "\n",
    "# --- Step 2: Export Rules to Excel ---\n",
    "with pd.ExcelWriter('Top_Association_Rules.xlsx') as writer:\n",
    "    for sheet_name, rules_df in rules_to_export.items():\n",
    "        if rules_df is not None and not rules_df.empty:\n",
    "            # Sort the rules by lift and confidence and take the top 10\n",
    "            rules_df_sorted = rules_df.sort_values(by=['lift', 'confidence'], ascending=False)\n",
    "            rules_df_sorted.head(10)[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        else:\n",
    "            # Create a simple DataFrame for segments with no rules\n",
    "            no_rules_df = pd.DataFrame([['No rules found for this segment.']], columns=['Info'])\n",
    "            no_rules_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"\\nTop 10 association rules for each category have been saved to 'Top_Association_Rules.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17b31ecd-b7ad-4744-bf27-0ae7523acf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmented into: Registered (665090), Guest (145919), General Fallback (1551)\n",
      "\n",
      "--- Training for Registered segment ---\n",
      "Generated 464 rules for Registered.\n",
      "Validation complete. Recall@3 for Registered is: 0.1994\n",
      "\n",
      "--- Training for Guest segment ---\n",
      "Generated 478 rules for Guest.\n",
      "Validation complete. Recall@3 for Guest is: 0.2081\n",
      "\n",
      "--- Training for General Fallback segment ---\n",
      "Generated 54 rules for General Fallback.\n",
      "Validation complete. Recall@3 for General Fallback is: 0.1910\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Preparation and Segmentation ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean customer type for consistency\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "\n",
    "# Filter for orders with at least two items to simulate a missing item\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "# Segment the data by 'CUSTOMER_TYPE'\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "\n",
    "# General fallback data includes all other customers (eClub, etc.)\n",
    "main_types = ['registered', 'guest']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "print(f\"Data segmented into: Registered ({len(registered_data)}), Guest ({len(guest_data)}), General Fallback ({len(general_data)})\")\n",
    "\n",
    "# --- Step 2: Helper Functions for Training and Validation ---\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "def calculate_recall_at_k(rules, test_transactions, k=3):\n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for order in test_transactions:\n",
    "        if len(order) < 2:\n",
    "            continue\n",
    "        missing_item = order[-1]\n",
    "        simulated_cart = order[:-1]\n",
    "        total_queries += 1\n",
    "        recommendations = get_recommendations(simulated_cart, rules, k)\n",
    "        if missing_item in recommendations:\n",
    "            correct_predictions += 1\n",
    "    recall_score = correct_predictions / total_queries if total_queries > 0 else 0\n",
    "    return recall_score\n",
    "\n",
    "def train_and_validate_segment(segment_df, segment_name, min_support, min_confidence):\n",
    "    print(f\"\\n--- Training for {segment_name} segment ---\")\n",
    "    if len(segment_df) < 100:\n",
    "        print(\"Not enough data to train. Skipping this segment.\")\n",
    "        return None, None\n",
    "\n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    train_transactions, test_transactions = train_test_split(transactions, test_size=0.3, random_state=42)\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_train = te.fit(train_transactions).transform(train_transactions)\n",
    "    df_one_hot_train = pd.DataFrame(te_ary_train, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot_train, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules for {segment_name}.\")\n",
    "\n",
    "    recall_score = calculate_recall_at_k(rules, test_transactions)\n",
    "    print(f\"Validation complete. Recall@3 for {segment_name} is: {recall_score:.4f}\")\n",
    "    return rules, recall_score\n",
    "\n",
    "# --- Step 3: Run Training and Validation for Each Segment ---\n",
    "# Use the optimal parameters we found earlier\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.0035\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "registered_rules, registered_score = train_and_validate_segment(registered_data, 'Registered', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "guest_rules, guest_score = train_and_validate_segment(guest_data, 'Guest', min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "general_rules, general_score = train_and_validate_segment(general_data, 'General Fallback', min_support=min_support_small, min_confidence=min_confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806e9889-a199-40cd-978c-b6f122bc9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final models trained for all segments. Ready for predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ got an unexpected keyword argument 's'. Support for arbitrary keyword arguments is deprecated and will be removed in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\ronak\\anaconda3\\Lib\\ast.py:602: DeprecationWarning: Constant.__init__ missing 1 required positional argument: 'value'. This will become an error in Python 3.15.\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'YourTeamName_Recommendation Output Sheet.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 141\u001b[0m\n\u001b[0;32m    136\u001b[0m     submission_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRECOMMENDATION_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data_question[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[i] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m i \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     )\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to an Excel file\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m submission_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYourTeamName_Recommendation Output Sheet.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal submission file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYourTeamName_Recommendation Output Sheet.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:2417\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2404\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2406\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2407\u001b[0m     df,\n\u001b[0;32m   2408\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2416\u001b[0m )\n\u001b[1;32m-> 2417\u001b[0m formatter\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m   2418\u001b[0m     excel_writer,\n\u001b[0;32m   2419\u001b[0m     sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[0;32m   2420\u001b[0m     startrow\u001b[38;5;241m=\u001b[39mstartrow,\n\u001b[0;32m   2421\u001b[0m     startcol\u001b[38;5;241m=\u001b[39mstartcol,\n\u001b[0;32m   2422\u001b[0m     freeze_panes\u001b[38;5;241m=\u001b[39mfreeze_panes,\n\u001b[0;32m   2423\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m   2424\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2425\u001b[0m     engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m   2426\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ExcelWriter(\n\u001b[0;32m    944\u001b[0m         writer,\n\u001b[0;32m    945\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    946\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    947\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    948\u001b[0m     )\n\u001b[0;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:61\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m     path,\n\u001b[0;32m     63\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m     64\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m     65\u001b[0m     if_sheet_exists\u001b[38;5;241m=\u001b[39mif_sheet_exists,\n\u001b[0;32m     66\u001b[0m     engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# the file and later write to it\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode:  \u001b[38;5;66;03m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1246\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[0;32m   1243\u001b[0m     cast(IO[\u001b[38;5;28mbytes\u001b[39m], path), compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m   1244\u001b[0m )\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1247\u001b[0m         path, mode, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'YourTeamName_Recommendation Output Sheet.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Data Loading and Final Model Training ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean relevant columns\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "\n",
    "# FIX: Clean item names in the training data before creating the ITEMS_LIST\n",
    "for col in item_name_columns:\n",
    "    final_dataset[col] = final_dataset[col].str.lower().str.strip()\n",
    "\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "final_dataset['ORDER_CREATED_DATE'] = pd.to_datetime(final_dataset['ORDER_CREATED_DATE'])\n",
    "final_dataset['ORDER_OCCASION_NAME'] = final_dataset['ORDER_OCCASION_NAME'].str.lower()\n",
    "\n",
    "# Filter for orders with at least two items\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "\n",
    "# --- Helper Functions for Training and Prediction ---\n",
    "def train_segment_model(segment_df, min_support, min_confidence):\n",
    "    \"\"\"Trains a model and returns the rules DataFrame.\"\"\"\n",
    "    if len(segment_df) < 100:\n",
    "        return None\n",
    "    \n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df_one_hot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "    return rules\n",
    "\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    \"\"\"Generates top-k recommendations based on association rules.\"\"\"\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "# Store the final rules in a dictionary\n",
    "final_rules_dict = {}\n",
    "min_support_large = 0.0005\n",
    "min_support_small = 0.0035\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "# Segment the data and train the final models on the entire segments\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "main_types = ['registered', 'guest']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "\n",
    "reg_togo_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "reg_del_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "guest_togo_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "guest_del_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "\n",
    "final_rules_dict['registered_togo'] = train_segment_model(reg_togo_data, min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "final_rules_dict['registered_delivery'] = train_segment_model(reg_del_data, min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "final_rules_dict['guest_togo'] = train_segment_model(guest_togo_data, min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "final_rules_dict['guest_delivery'] = train_segment_model(guest_del_data, min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "final_rules_dict['general'] = train_segment_model(general_data, min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "\n",
    "\n",
    "print(\"Final models trained for all segments. Ready for predictions.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Prediction on the Test Data ---\n",
    "try:\n",
    "    test_data_question = pd.read_csv('test_data_question.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'test_data_question.csv' not found. Please ensure the file is in the directory.\")\n",
    "    exit()\n",
    "\n",
    "# Prepare the test data by creating a list of items in each cart\n",
    "item_columns_test = [col for col in test_data_question.columns if 'item' in col.lower()]\n",
    "test_data_question['cart_items'] = test_data_question[item_columns_test].apply(\n",
    "    lambda row: [str(item).lower().strip() for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Clean the customer type and order occasion columns directly from the test data\n",
    "test_data_question['CUSTOMER_TYPE'] = test_data_question['CUSTOMER_TYPE'].str.lower()\n",
    "test_data_question['ORDER_OCCASION_NAME'] = test_data_question['ORDER_OCCASION_NAME'].str.lower()\n",
    "test_data_question['CUSTOMER_TYPE'] = test_data_question['CUSTOMER_TYPE'].fillna('unknown')\n",
    "\n",
    "\n",
    "# --- Step 3: Generate Recommendations based on Segments ---\n",
    "def get_segmented_recommendations(row, rules_dict):\n",
    "    customer_type = str(row['CUSTOMER_TYPE']).lower()\n",
    "    order_occasion = str(row['ORDER_OCCASION_NAME']).lower()\n",
    "    cart_items = row['cart_items']\n",
    "    \n",
    "    # Try to get rules for the specific segment\n",
    "    key = f\"{customer_type}_{order_occasion}\"\n",
    "    rules = rules_dict.get(key)\n",
    "    \n",
    "    # If no specific rules found, use the general fallback model\n",
    "    if rules is None:\n",
    "        rules = rules_dict.get('general')\n",
    "        \n",
    "    if rules is not None:\n",
    "        return get_recommendations(cart_items, rules)\n",
    "    else:\n",
    "        return [None, None, None]\n",
    "\n",
    "test_data_question['recommendations'] = test_data_question.apply(get_segmented_recommendations, axis=1, rules_dict=final_rules_dict)\n",
    "\n",
    "\n",
    "# --- Step 4: Final Output Generation ---\n",
    "submission_df = test_data_question[['CUSTOMER_ID', 'ORDER_ID']].copy()\n",
    "for i in range(3):\n",
    "    submission_df[f'RECOMMENDATION_{i+1}'] = test_data_question['recommendations'].apply(\n",
    "        lambda x: x[i] if len(x) > i else None\n",
    "    )\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "submission_df.to_excel('YourTeamName_Recommendation Output Sheet.xlsx', index=False)\n",
    "print(\"\\nFinal submission file 'YourTeamName_Recommendation Output Sheet.xlsx' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8f4225-b3d5-47a7-850f-e620c069fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final models trained for all segments. Ready for predictions.\n",
      "\n",
      "Final submission file 'YourTeamName_Recommendation Output Sheet.xlsx' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# --- Step 1: Data Loading and Final Model Training ---\n",
    "try:\n",
    "    final_dataset = pd.read_csv('final_dataset_with_all_features.csv', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset_with_all_features.csv' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Recreate the 'ITEMS_LIST' column and clean relevant columns\n",
    "item_name_columns = [col for col in final_dataset.columns if 'item' in col.lower() and 'name' in col.lower()]\n",
    "\n",
    "# Clean item names in the training data before creating the ITEMS_LIST\n",
    "for col in item_name_columns:\n",
    "    final_dataset[col] = final_dataset[col].str.lower().str.strip()\n",
    "\n",
    "final_dataset['ITEMS_LIST'] = final_dataset[item_name_columns].apply(\n",
    "    lambda row: [item for item in row.dropna()], axis=1\n",
    ")\n",
    "final_dataset['CUSTOMER_TYPE'] = final_dataset['CUSTOMER_TYPE'].str.lower()\n",
    "final_dataset['ORDER_CREATED_DATE'] = pd.to_datetime(final_dataset['ORDER_CREATED_DATE'])\n",
    "final_dataset['ORDER_OCCASION_NAME'] = final_dataset['ORDER_OCCASION_NAME'].str.lower()\n",
    "\n",
    "# Filter for orders with at least two items\n",
    "final_dataset_filtered = final_dataset[final_dataset['ITEMS_LIST'].str.len() > 1].copy()\n",
    "\n",
    "\n",
    "# --- Helper Functions for Training and Prediction ---\n",
    "def train_segment_model(segment_df, min_support, min_confidence):\n",
    "    \"\"\"Trains a model and returns the rules DataFrame.\"\"\"\n",
    "    if len(segment_df) < 100:\n",
    "        return None\n",
    "    \n",
    "    transactions = segment_df['ITEMS_LIST'].tolist()\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df_one_hot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df_one_hot, min_support=min_support, max_len=3, low_memory=True, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules.sort_values(by=['lift', 'confidence'], ascending=False, inplace=True)\n",
    "    return rules\n",
    "\n",
    "def get_recommendations(cart_items, rules_df, k=3):\n",
    "    \"\"\"Generates top-k recommendations based on association rules.\"\"\"\n",
    "    recommendations = set()\n",
    "    for item in cart_items:\n",
    "        matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: item in x)]\n",
    "        if not matching_rules.empty:\n",
    "            for _, rule_row in matching_rules.iterrows():\n",
    "                consequent_items = list(rule_row['consequents'])\n",
    "                for rec_item in consequent_items:\n",
    "                    if rec_item not in cart_items:\n",
    "                        recommendations.add(rec_item)\n",
    "                        if len(recommendations) >= k:\n",
    "                            return list(recommendations)\n",
    "    return list(recommendations)[:k]\n",
    "\n",
    "# Store the final rules in a dictionary\n",
    "final_rules_dict = {}\n",
    "min_support_large = 0.0001\n",
    "min_support_small = 0.0035\n",
    "min_confidence_val = 0.4\n",
    "\n",
    "# Segment the data and train the final models on the entire segments\n",
    "registered_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'registered']\n",
    "guest_data = final_dataset_filtered[final_dataset_filtered['CUSTOMER_TYPE'] == 'guest']\n",
    "main_types = ['registered', 'guest']\n",
    "general_data = final_dataset_filtered[~final_dataset_filtered['CUSTOMER_TYPE'].isin(main_types)].copy()\n",
    "\n",
    "\n",
    "reg_togo_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "reg_del_data = registered_data[registered_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "guest_togo_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'togo']\n",
    "guest_del_data = guest_data[guest_data['ORDER_OCCASION_NAME'] == 'delivery']\n",
    "\n",
    "\n",
    "final_rules_dict['registered_togo'] = train_segment_model(reg_togo_data, min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "final_rules_dict['registered_delivery'] = train_segment_model(reg_del_data, min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "final_rules_dict['guest_togo'] = train_segment_model(guest_togo_data, min_support=min_support_large, min_confidence=min_confidence_val)\n",
    "final_rules_dict['guest_delivery'] = train_segment_model(guest_del_data, min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "final_rules_dict['general'] = train_segment_model(general_data, min_support=min_support_small, min_confidence=min_confidence_val)\n",
    "\n",
    "\n",
    "print(\"Final models trained for all segments. Ready for predictions.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Prediction on the Test Data ---\n",
    "try:\n",
    "    test_data_question = pd.read_csv('test_data_question.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'test_data_question.csv' not found. Please ensure the file is in the directory.\")\n",
    "    exit()\n",
    "\n",
    "# Prepare the test data by creating a list of items in each cart\n",
    "item_columns_test = [col for col in test_data_question.columns if 'item' in col.lower()]\n",
    "test_data_question['cart_items'] = test_data_question[item_columns_test].apply(\n",
    "    lambda row: [str(item).lower().strip() for item in row.dropna()], axis=1\n",
    ")\n",
    "\n",
    "# Clean the customer type and order occasion columns directly from the test data\n",
    "test_data_question['CUSTOMER_TYPE'] = test_data_question['CUSTOMER_TYPE'].str.lower()\n",
    "test_data_question['ORDER_OCCASION_NAME'] = test_data_question['ORDER_OCCASION_NAME'].str.lower()\n",
    "test_data_question['CUSTOMER_TYPE'] = test_data_question['CUSTOMER_TYPE'].fillna('unknown')\n",
    "\n",
    "\n",
    "# --- Step 3: Generate Recommendations based on Segments ---\n",
    "def get_segmented_recommendations(row, rules_dict):\n",
    "    customer_type = str(row['CUSTOMER_TYPE']).lower()\n",
    "    order_occasion = str(row['ORDER_OCCASION_NAME']).lower()\n",
    "    cart_items = row['cart_items']\n",
    "    \n",
    "    # Try to get rules for the specific segment\n",
    "    key = f\"{customer_type}_{order_occasion}\"\n",
    "    rules = rules_dict.get(key)\n",
    "    \n",
    "    # If no specific rules found, use the general fallback model\n",
    "    if rules is None:\n",
    "        rules = rules_dict.get('general')\n",
    "        \n",
    "    if rules is not None:\n",
    "        return get_recommendations(cart_items, rules)\n",
    "    else:\n",
    "        return [None, None, None]\n",
    "\n",
    "test_data_question['recommendations'] = test_data_question.apply(get_segmented_recommendations, axis=1, rules_dict=final_rules_dict)\n",
    "\n",
    "\n",
    "# --- Step 4: Final Output Generation ---\n",
    "submission_df = test_data_question[['CUSTOMER_ID', 'ORDER_ID']].copy()\n",
    "for i in range(3):\n",
    "    submission_df[f'RECOMMENDATION_{i+1}'] = test_data_question['recommendations'].apply(\n",
    "        lambda x: x[i] if len(x) > i else None\n",
    "    )\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "submission_df.to_excel('YourTeamName_Recommendation Output Sheet.xlsx', index=False)\n",
    "print(\"\\nFinal submission file 'YourTeamName_Recommendation Output Sheet.xlsx' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7ea04-24dd-45c8-a17e-f65987e06546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
